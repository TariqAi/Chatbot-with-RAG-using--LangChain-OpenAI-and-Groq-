{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC6OkxRbNSPH",
        "outputId": "47ccf485-1340-460b-e16d-a0f82144a53d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.8-py3-none-any.whl (987 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.6/987.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.35.14-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting qdrant-client\n",
            "  Downloading qdrant_client-1.10.1-py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.16-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.19 (from langchain)\n",
            "  Downloading langchain_core-0.2.19-py3-none-any.whl (366 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.5/366.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.86-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.64.1)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client)\n",
            "  Downloading grpcio_tools-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client)\n",
            "  Downloading protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (67.7.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2<5,>=3 (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.19->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx<1,>=0.23.0->openai)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx<1,>=0.23.0->openai)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.19->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, python-dotenv, pyarrow, protobuf, portalocker, orjson, jsonpointer, hyperframe, hpack, h11, dill, tiktoken, multiprocess, jsonpatch, httpcore, h2, grpcio-tools, langsmith, httpx, openai, langchain-core, datasets, qdrant-client, langchain-text-splitters, langchain_openai, langchain\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.2 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-api-core 2.16.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-aiplatform 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-bigtable 2.24.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.2 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 grpcio-tools-1.64.1 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.5 httpx-0.27.0 hyperframe-6.0.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.8 langchain-core-0.2.19 langchain-text-splitters-0.2.2 langchain_openai-0.1.16 langsmith-0.1.86 multiprocess-0.70.16 openai-1.35.14 orjson-3.10.6 portalocker-2.10.1 protobuf-5.27.2 pyarrow-17.0.0 python-dotenv-1.0.1 qdrant-client-1.10.1 requests-2.32.3 tiktoken-0.7.0 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U   \\\n",
        "    langchain     \\\n",
        "    openai        \\\n",
        "    datasets      \\\n",
        "    qdrant-client \\\n",
        "    tiktoken      \\\n",
        "    python-dotenv \\\n",
        "    langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "load_dotenv('./.env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6174Nwf2Nk3O",
        "outputId": "33a24090-318d-4785-97d7-eea3193892be"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(model='gpt-3.5-turbo', openai_api_key='OPENAI_API_KEY')\n"
      ],
      "metadata": {
        "id": "gzIHL4c-NlWI"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
        "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
        "    HumanMessage(content=\"I'd like to understand data science\")\n",
        "]"
      ],
      "metadata": {
        "id": "lgpy_3IFNlYg"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = chat.invoke(messages)\n",
        "res"
      ],
      "metadata": {
        "id": "aO5T4THFNlbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0d51d6-477e-4d01-9b09-f5fa43b1151c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure! Data science is a multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It combines elements of statistics, computer science, machine learning, and domain knowledge to analyze and interpret complex data sets.\\n\\nData scientists use various tools and techniques to clean, organize, analyze, and visualize data to extract valuable insights and make informed decisions. They often work with large data sets to uncover patterns, trends, and correlations that can help businesses and organizations improve their operations, products, and services.\\n\\nIf you have any specific questions or topics you'd like to learn more about in data science, feel free to ask!\", response_metadata={'token_usage': {'completion_tokens': 136, 'prompt_tokens': 52, 'total_tokens': 188}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3268f26e-6ded-4bfc-a903-ec09fd416694-0', usage_metadata={'input_tokens': 52, 'output_tokens': 136, 'total_tokens': 188})"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res.content)"
      ],
      "metadata": {
        "id": "v2y4-VoGNlfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70dec00-21bc-4286-d87f-9c3669b7eab5"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Data science is a multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It combines elements of statistics, computer science, machine learning, and domain knowledge to analyze and interpret complex data sets.\n",
            "\n",
            "Data scientists use various tools and techniques to clean, organize, analyze, and visualize data to extract valuable insights and make informed decisions. They often work with large data sets to uncover patterns, trends, and correlations that can help businesses and organizations improve their operations, products, and services.\n",
            "\n",
            "If you have any specific questions or topics you'd like to learn more about in data science, feel free to ask!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(res)"
      ],
      "metadata": {
        "id": "tILbS5HGNlic"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf8Fvn9-2LE0",
        "outputId": "b7bc17fb-f3f8-4eeb-a079-19229c3a0c5e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='Hi AI, how are you today?'), AIMessage(content=\"I'm great thank you. How can I help you?\"), HumanMessage(content=\"I'd like to understand data science\"), AIMessage(content=\"Sure! Data science is a multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It combines elements of statistics, computer science, machine learning, and domain knowledge to analyze and interpret complex data sets.\\n\\nData scientists use various tools and techniques to clean, organize, analyze, and visualize data to extract valuable insights and make informed decisions. They often work with large data sets to uncover patterns, trends, and correlations that can help businesses and organizations improve their operations, products, and services.\\n\\nIf you have any specific questions or topics you'd like to learn more about in data science, feel free to ask!\", response_metadata={'token_usage': {'completion_tokens': 136, 'prompt_tokens': 52, 'total_tokens': 188}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3268f26e-6ded-4bfc-a903-ec09fd416694-0', usage_metadata={'input_tokens': 52, 'output_tokens': 136, 'total_tokens': 188})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=\"Whats the difference between supervised and unsupervised?\"\n",
        ")\n",
        "messages.append(prompt)"
      ],
      "metadata": {
        "id": "0wdbYXEv2LCY"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = chat.invoke(messages)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es0ADvDm2LAG",
        "outputId": "0415dd5e-2a27-4fd4-f81f-ff097ac72e0a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Supervised and unsupervised learning are two main types of machine learning algorithms that are used to train models to make predictions or discover patterns in data.\\n\\n1. Supervised Learning:\\n- In supervised learning, the algorithm is trained on a labeled dataset, where both the input data and the corresponding output or target labels are provided.\\n- The goal of supervised learning is to learn a mapping function from the input data to the output labels, so that the model can make predictions on new, unseen data.\\n- Common supervised learning tasks include classification, where the goal is to predict a discrete label or category for each input data point, and regression, where the goal is to predict a continuous value.\\n- Examples of supervised learning algorithms include linear regression, logistic regression, support vector machines (SVM), decision trees, and neural networks.\\n\\n2. Unsupervised Learning:\\n- In unsupervised learning, the algorithm is trained on an unlabeled dataset, where only the input data is provided without any corresponding output labels.\\n- The goal of unsupervised learning is to find patterns, structure, or relationships in the data without explicit guidance or supervision.\\n- Common unsupervised learning tasks include clustering, where the goal is to group similar data points together based on their characteristics, and dimensionality reduction, where the goal is to reduce the number of features in the data while preserving its important information.\\n- Examples of unsupervised learning algorithms include k-means clustering, hierarchical clustering, principal component analysis (PCA), and t-distributed stochastic neighbor embedding (t-SNE).\\n\\nIn summary, supervised learning requires labeled data for training, while unsupervised learning does not. Supervised learning is used for tasks where the goal is to make predictions or classify data into predefined categories, while unsupervised learning is used for tasks such as clustering and dimensionality reduction.', response_metadata={'token_usage': {'completion_tokens': 368, 'prompt_tokens': 206, 'total_tokens': 574}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-356f659a-fcda-4fa8-b886-3ee4008b15dd-0', usage_metadata={'input_tokens': 206, 'output_tokens': 368, 'total_tokens': 574})"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so4A9sgu2K9y",
        "outputId": "da14ac61-e6d7-4b48-80af-3d3d4329b2d9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supervised and unsupervised learning are two main types of machine learning algorithms that are used to train models to make predictions or discover patterns in data.\n",
            "\n",
            "1. Supervised Learning:\n",
            "- In supervised learning, the algorithm is trained on a labeled dataset, where both the input data and the corresponding output or target labels are provided.\n",
            "- The goal of supervised learning is to learn a mapping function from the input data to the output labels, so that the model can make predictions on new, unseen data.\n",
            "- Common supervised learning tasks include classification, where the goal is to predict a discrete label or category for each input data point, and regression, where the goal is to predict a continuous value.\n",
            "- Examples of supervised learning algorithms include linear regression, logistic regression, support vector machines (SVM), decision trees, and neural networks.\n",
            "\n",
            "2. Unsupervised Learning:\n",
            "- In unsupervised learning, the algorithm is trained on an unlabeled dataset, where only the input data is provided without any corresponding output labels.\n",
            "- The goal of unsupervised learning is to find patterns, structure, or relationships in the data without explicit guidance or supervision.\n",
            "- Common unsupervised learning tasks include clustering, where the goal is to group similar data points together based on their characteristics, and dimensionality reduction, where the goal is to reduce the number of features in the data while preserving its important information.\n",
            "- Examples of unsupervised learning algorithms include k-means clustering, hierarchical clustering, principal component analysis (PCA), and t-distributed stochastic neighbor embedding (t-SNE).\n",
            "\n",
            "In summary, supervised learning requires labeled data for training, while unsupervised learning does not. Supervised learning is used for tasks where the goal is to make predictions or classify data into predefined categories, while unsupervised learning is used for tasks such as clustering and dimensionality reduction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add latest response to messages\n",
        "messages.append(res)\n",
        "\n",
        "# create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=\"What is so special about Mistral 7B?\"\n",
        ")\n",
        "# append to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "# send to GPT\n",
        "res = chat.invoke(messages)"
      ],
      "metadata": {
        "id": "Rnw4QPey2K7Z"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awdaJ80F2K5O",
        "outputId": "5cfcdef1-8949-466b-a225-bf9033811c30"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistral 7B is a well-known climbing route located in the Verdon Gorge in France. It is considered one of the most iconic and challenging climbing routes in the world, attracting experienced climbers from around the globe.\n",
            "\n",
            "Some key aspects that make Mistral 7B special include:\n",
            "\n",
            "1. Difficulty: Mistral 7B is known for its high level of difficulty, with technical climbing moves, steep overhangs, and challenging sections that require advanced climbing skills and strength.\n",
            "\n",
            "2. Length: The route is quite long, typically consisting of multiple pitches that climbers need to ascend one after another. This adds to the physical and mental challenge of the climb.\n",
            "\n",
            "3. Scenery: The Verdon Gorge is renowned for its stunning natural beauty, with towering limestone cliffs, turquoise waters of the Verdon River below, and breathtaking views of the surrounding landscape. Climbers on Mistral 7B are treated to a spectacular and awe-inspiring environment as they make their way up the route.\n",
            "\n",
            "4. Reputation: Mistral 7B has a reputation as a classic and must-do climb for serious climbers seeking a challenging and rewarding experience. Its combination of technical difficulty, length, and striking setting make it a coveted climb for those looking to test their skills and push their limits.\n",
            "\n",
            "Overall, Mistral 7B stands out as a special climbing route due to its combination of challenging features, beautiful surroundings, and legendary status within the climbing community.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add latest response to messages\n",
        "messages.append(res)\n",
        "\n",
        "# create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=\"Can you tell me about the LLMChain in LangChain?\"\n",
        ")\n",
        "# append to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "# send to GPT\n",
        "res = chat.invoke(messages)"
      ],
      "metadata": {
        "id": "ZvAaOYVp2K3A"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hOxKIls4EDo",
        "outputId": "7ae35cd9-d6e9-4259-850e-ed50e74bfdc8"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm not familiar with specific technologies or terms like \"LLMChain\" in the context of LangChain. It's possible that you may be referring to a specific blockchain technology or concept that I'm not aware of. If you can provide more context or clarify your question, I'd be happy to try to help you further.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llmchain_information = [\n",
        "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
        "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
        "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
        "]\n",
        "\n",
        "source_knowledge = \"\\n\".join(llmchain_information)"
      ],
      "metadata": {
        "id": "Hf3RS2hY4D9K"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
        "\n",
        "augmented_prompt = f\"\"\"Using the contexts below to answer the question.\n",
        "\n",
        "Contexts:\n",
        "{source_knowledge}\n",
        "\n",
        "Question: {query}\"\"\""
      ],
      "metadata": {
        "id": "p_JTcVNO4D66"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augmented_prompt\n",
        ")\n",
        "\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat.invoke(messages)"
      ],
      "metadata": {
        "id": "L8CoBecY4D4W"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xZy2Oae4D2G",
        "outputId": "d415954b-6bb0-4dd0-d42a-1e0f24de3ece"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LLMChain is a common type of chain in the LangChain framework for developing applications powered by language models. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. The LLMChain takes multiple input variables, uses the PromptTemplate to format them into a prompt, passes that prompt to the model for processing, and then uses the OutputParser (if provided) to parse the output of the LLM into a final format.\n",
            "\n",
            "In the context of LangChain, chains are a generic concept that refers to a sequence of modular components (or other chains) combined in a particular way to achieve a common use case. The LangChain framework aims to enable the development of applications that go beyond simply calling out to a language model via an API. These applications are designed to be data-aware, connecting language models to other sources of data, and agentic, allowing language models to interact with their environment.\n",
            "\n",
            "Overall, the LLMChain in LangChain represents a key component that leverages language models within a chain of processes to create powerful and differentiated applications that can interact with data sources and environments, enhancing their functionality and capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key='OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "NLZSHaJv6gqU"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    'this is one chunk',\n",
        "    'this is the second chunk of text'\n",
        "]\n",
        "\n",
        "res = embed_model.embed_documents(texts)\n",
        "len(res), len(res[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-JEUQBmCuV2",
        "outputId": "dc2d7828-6100-47c2-ffcc-d65c7ce2b107"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1536)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jfu6iPWCuPQ",
        "outputId": "9a55b137-f375-4e98-a648-738b952a0b1e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0071220616810023785,\n",
              "  -0.011852039955556393,\n",
              "  0.0066188727505505085,\n",
              "  0.033071137964725494,\n",
              "  -0.0189044289290905,\n",
              "  -0.020746875554323196,\n",
              "  0.025918111205101013,\n",
              "  -0.012378453277051449,\n",
              "  -0.030686795711517334,\n",
              "  0.012982279993593693,\n",
              "  0.09593107551336288,\n",
              "  -0.030160382390022278,\n",
              "  -0.041989199817180634,\n",
              "  -0.047439124435186386,\n",
              "  0.014654415659606457,\n",
              "  0.05307484418153763,\n",
              "  -0.009630265645682812,\n",
              "  -0.0018898623529821634,\n",
              "  -0.039790648967027664,\n",
              "  0.004857710562646389,\n",
              "  0.011294661089777946,\n",
              "  -0.020901702344417572,\n",
              "  0.035145826637744904,\n",
              "  0.026150353252887726,\n",
              "  -0.0251749400049448,\n",
              "  -0.006185355596244335,\n",
              "  -0.008438094519078732,\n",
              "  0.015838846564292908,\n",
              "  0.022960906848311424,\n",
              "  -0.011310143396258354,\n",
              "  -0.03026876226067543,\n",
              "  -0.040719613432884216,\n",
              "  0.014329278841614723,\n",
              "  -0.04660305753350258,\n",
              "  0.031244175508618355,\n",
              "  -0.027358006685972214,\n",
              "  -0.026800628751516342,\n",
              "  0.035517413169145584,\n",
              "  0.01532791554927826,\n",
              "  -0.01214621216058731,\n",
              "  0.023781493306159973,\n",
              "  -0.03963582217693329,\n",
              "  0.009189007803797722,\n",
              "  -0.0019450195832177997,\n",
              "  0.015606604516506195,\n",
              "  0.009189007803797722,\n",
              "  -0.058369942009449005,\n",
              "  0.020143048837780952,\n",
              "  -0.02223321795463562,\n",
              "  0.053539324551820755,\n",
              "  0.02901466004550457,\n",
              "  0.04737719148397446,\n",
              "  -0.07060130685567856,\n",
              "  0.00819811224937439,\n",
              "  -0.014313795603811741,\n",
              "  0.005345417186617851,\n",
              "  -0.006235674489289522,\n",
              "  0.06620420515537262,\n",
              "  0.022883493453264236,\n",
              "  0.0010237963870167732,\n",
              "  0.03687989339232445,\n",
              "  -0.03273051977157593,\n",
              "  -0.008832904510200024,\n",
              "  -0.021490046754479408,\n",
              "  0.0036074791569262743,\n",
              "  0.008337456732988358,\n",
              "  -0.06121876463294029,\n",
              "  0.01744905114173889,\n",
              "  0.023843424394726753,\n",
              "  0.02474142238497734,\n",
              "  0.022031942382454872,\n",
              "  -0.019446324557065964,\n",
              "  -0.05298194661736488,\n",
              "  -0.000418517884099856,\n",
              "  0.026847075670957565,\n",
              "  -0.009816058911383152,\n",
              "  -0.005627977196127176,\n",
              "  0.01627236232161522,\n",
              "  -0.003924875520169735,\n",
              "  0.009792834520339966,\n",
              "  7.922809891169891e-05,\n",
              "  -0.00635566608980298,\n",
              "  0.0011224988847970963,\n",
              "  -0.055614013224840164,\n",
              "  0.026599351316690445,\n",
              "  0.001152496668510139,\n",
              "  -0.10001851618289948,\n",
              "  0.015645312145352364,\n",
              "  -0.004788038320839405,\n",
              "  0.028983695432543755,\n",
              "  -0.02430790662765503,\n",
              "  -0.022248702123761177,\n",
              "  -0.04613857343792915,\n",
              "  0.018316084519028664,\n",
              "  0.044435471296310425,\n",
              "  -0.014112520031630993,\n",
              "  -0.04127699136734009,\n",
              "  -0.002703673904761672,\n",
              "  0.004644822794944048,\n",
              "  -0.0349290668964386,\n",
              "  0.018641222268342972,\n",
              "  -0.0076368628069758415,\n",
              "  -0.021381668746471405,\n",
              "  -0.004567408934235573,\n",
              "  0.028859833255410194,\n",
              "  -0.03734437748789787,\n",
              "  -0.015018260106444359,\n",
              "  0.03561030700802803,\n",
              "  -0.04647919535636902,\n",
              "  -0.01909022219479084,\n",
              "  -0.017216810956597328,\n",
              "  0.02663031779229641,\n",
              "  -0.02054559998214245,\n",
              "  0.01839349791407585,\n",
              "  0.024323388934135437,\n",
              "  0.011503677815198898,\n",
              "  0.01853284239768982,\n",
              "  -0.01625688001513481,\n",
              "  0.005701520014554262,\n",
              "  0.013005504384636879,\n",
              "  -0.004989313893020153,\n",
              "  -0.0025082044303417206,\n",
              "  -0.03684892877936363,\n",
              "  -0.044930920004844666,\n",
              "  -0.007547837216407061,\n",
              "  -0.034185897558927536,\n",
              "  0.05908214673399925,\n",
              "  -0.04815132915973663,\n",
              "  -0.021087495610117912,\n",
              "  0.010040558874607086,\n",
              "  0.00435065058991313,\n",
              "  -0.016334293410182,\n",
              "  -0.014832467772066593,\n",
              "  -0.011348850093781948,\n",
              "  -0.042732369154691696,\n",
              "  -0.006181485019624233,\n",
              "  -0.03378334641456604,\n",
              "  0.007683311123400927,\n",
              "  -0.03141448646783829,\n",
              "  0.04647919535636902,\n",
              "  6.0207305068615824e-05,\n",
              "  -0.03229700028896332,\n",
              "  -0.011124351061880589,\n",
              "  -0.029401728883385658,\n",
              "  -0.015444035641849041,\n",
              "  0.024633044376969337,\n",
              "  -0.03610575571656227,\n",
              "  0.053229670971632004,\n",
              "  -0.014623451046645641,\n",
              "  -0.03242086246609688,\n",
              "  0.035145826637744904,\n",
              "  -0.018052877858281136,\n",
              "  -0.044311609119176865,\n",
              "  -0.00013583687541540712,\n",
              "  -0.0349290668964386,\n",
              "  -0.02297639101743698,\n",
              "  0.013508693315088749,\n",
              "  -0.013237745501101017,\n",
              "  -0.04075057804584503,\n",
              "  0.03938809782266617,\n",
              "  -0.02977331541478634,\n",
              "  -0.064532071352005,\n",
              "  0.03198734670877457,\n",
              "  0.026831593364477158,\n",
              "  -0.0014544101431965828,\n",
              "  0.042546577751636505,\n",
              "  -0.041555680334568024,\n",
              "  0.003746823873370886,\n",
              "  -0.07134447991847992,\n",
              "  -0.025097526609897614,\n",
              "  -0.03208024427294731,\n",
              "  0.01964760012924671,\n",
              "  -0.0423298180103302,\n",
              "  -0.04257754236459732,\n",
              "  -0.013485468924045563,\n",
              "  -0.01374867558479309,\n",
              "  -0.020700426772236824,\n",
              "  -0.049018364399671555,\n",
              "  0.009537369944155216,\n",
              "  -0.006061493884772062,\n",
              "  0.020994599908590317,\n",
              "  0.043816160410642624,\n",
              "  0.0039035866502672434,\n",
              "  0.021381668746471405,\n",
              "  -0.05809124931693077,\n",
              "  0.0075517077930271626,\n",
              "  0.017913533374667168,\n",
              "  0.025407180190086365,\n",
              "  0.023951802402734756,\n",
              "  0.007516871672123671,\n",
              "  -0.04000740870833397,\n",
              "  0.03660120442509651,\n",
              "  -0.026738697662949562,\n",
              "  0.029277866706252098,\n",
              "  0.011163057759404182,\n",
              "  -0.03489810228347778,\n",
              "  0.10100941359996796,\n",
              "  0.01853284239768982,\n",
              "  0.044559333473443985,\n",
              "  -0.002318540820851922,\n",
              "  0.0014689252711832523,\n",
              "  0.06837179511785507,\n",
              "  0.005515727214515209,\n",
              "  0.01638074219226837,\n",
              "  -0.009777352213859558,\n",
              "  -0.0118675222620368,\n",
              "  -0.049328017979860306,\n",
              "  0.0189044289290905,\n",
              "  0.010388920083642006,\n",
              "  -0.022248702123761177,\n",
              "  -0.0026456136256456375,\n",
              "  -0.02424597553908825,\n",
              "  -5.990490899421275e-05,\n",
              "  -0.02299187332391739,\n",
              "  0.007110449485480785,\n",
              "  -0.00109540403354913,\n",
              "  -0.013756416738033295,\n",
              "  0.018424464389681816,\n",
              "  -0.007729759439826012,\n",
              "  -0.009165783412754536,\n",
              "  0.02292994223535061,\n",
              "  0.015807880088686943,\n",
              "  0.06298379600048065,\n",
              "  0.0628909021615982,\n",
              "  -0.020684944465756416,\n",
              "  -0.01840898208320141,\n",
              "  0.018439946696162224,\n",
              "  -0.031166762113571167,\n",
              "  0.009026438929140568,\n",
              "  -0.0005970532656647265,\n",
              "  0.013764158822596073,\n",
              "  -0.009498662315309048,\n",
              "  -0.016845224425196648,\n",
              "  -0.03929520025849342,\n",
              "  -0.015552415512502193,\n",
              "  -0.011132092215120792,\n",
              "  -0.03434072434902191,\n",
              "  0.0038745563942939043,\n",
              "  -0.0035590955521911383,\n",
              "  -0.00632470054551959,\n",
              "  -0.050659533590078354,\n",
              "  -0.03824237361550331,\n",
              "  0.04836808890104294,\n",
              "  -0.005686037242412567,\n",
              "  -0.006793053355067968,\n",
              "  -0.023270562291145325,\n",
              "  -0.029432693496346474,\n",
              "  0.010272799991071224,\n",
              "  -0.017805153504014015,\n",
              "  0.055614013224840164,\n",
              "  0.03653927147388458,\n",
              "  -0.0038377849850803614,\n",
              "  -0.026614835485816002,\n",
              "  -0.0076213800348341465,\n",
              "  -0.061900004744529724,\n",
              "  -0.019260531291365623,\n",
              "  0.012239107862114906,\n",
              "  0.031739622354507446,\n",
              "  -0.007160768378525972,\n",
              "  -0.008554215542972088,\n",
              "  -0.008081992156803608,\n",
              "  0.008956766687333584,\n",
              "  -0.05638815090060234,\n",
              "  0.0526103600859642,\n",
              "  -0.06298379600048065,\n",
              "  -0.013222262263298035,\n",
              "  0.06663772463798523,\n",
              "  0.01783611997961998,\n",
              "  0.019771462306380272,\n",
              "  -0.006564682815223932,\n",
              "  0.010969523340463638,\n",
              "  0.009816058911383152,\n",
              "  0.04824422672390938,\n",
              "  -0.04508574679493904,\n",
              "  -0.0497305691242218,\n",
              "  0.011232730001211166,\n",
              "  0.02782248891890049,\n",
              "  0.018888946622610092,\n",
              "  -0.035950928926467896,\n",
              "  0.02368859574198723,\n",
              "  -0.01625688001513481,\n",
              "  -0.02724962681531906,\n",
              "  0.0033500785939395428,\n",
              "  -0.0005481858388520777,\n",
              "  -0.008770973421633244,\n",
              "  -0.02737348899245262,\n",
              "  0.040471889078617096,\n",
              "  0.0007093031308613718,\n",
              "  0.026614835485816002,\n",
              "  -0.0032533113844692707,\n",
              "  0.04282526671886444,\n",
              "  0.0028875316493213177,\n",
              "  0.042856231331825256,\n",
              "  -0.009777352213859558,\n",
              "  0.0016469768015667796,\n",
              "  0.009274163283407688,\n",
              "  -0.0456121601164341,\n",
              "  -0.004373874980956316,\n",
              "  0.020390773192048073,\n",
              "  -0.0029746219515800476,\n",
              "  0.0014466687571257353,\n",
              "  0.025918111205101013,\n",
              "  0.05192911997437477,\n",
              "  -0.013113883323967457,\n",
              "  0.010853402316570282,\n",
              "  0.07301661372184753,\n",
              "  0.020390773192048073,\n",
              "  0.013802865520119667,\n",
              "  -0.026769662275910378,\n",
              "  -0.0002871564938686788,\n",
              "  0.01789805106818676,\n",
              "  -0.008616146631538868,\n",
              "  -0.0008021995890885592,\n",
              "  -0.00769492331892252,\n",
              "  -0.017356155440211296,\n",
              "  -0.028983695432543755,\n",
              "  0.045426368713378906,\n",
              "  0.0019198601366952062,\n",
              "  0.02429242432117462,\n",
              "  -0.01422864105552435,\n",
              "  0.029757831245660782,\n",
              "  -0.022124839946627617,\n",
              "  -0.020932668820023537,\n",
              "  0.014290571212768555,\n",
              "  -0.019306980073451996,\n",
              "  0.00018446236208546907,\n",
              "  -0.0077220178209245205,\n",
              "  0.028410833328962326,\n",
              "  -0.0064214677549898624,\n",
              "  -0.028968211263418198,\n",
              "  -0.008089733310043812,\n",
              "  -0.016814257949590683,\n",
              "  -0.015916259959340096,\n",
              "  -0.005898925010114908,\n",
              "  -0.017294224351644516,\n",
              "  -0.04133892431855202,\n",
              "  -0.009994110092520714,\n",
              "  -0.014360244385898113,\n",
              "  0.0027791522443294525,\n",
              "  0.016473637893795967,\n",
              "  -0.026862557977437973,\n",
              "  -0.0604136623442173,\n",
              "  0.03684892877936363,\n",
              "  -0.024633044376969337,\n",
              "  -0.030872588977217674,\n",
              "  0.02529880218207836,\n",
              "  0.02085525542497635,\n",
              "  -0.01315259002149105,\n",
              "  -0.00222177361138165,\n",
              "  -0.01966308429837227,\n",
              "  -0.0010828243102878332,\n",
              "  0.018424464389681816,\n",
              "  -0.004060349427163601,\n",
              "  0.013446762226521969,\n",
              "  0.015188571065664291,\n",
              "  -0.040533822029829025,\n",
              "  -0.017681293189525604,\n",
              "  -0.04140085354447365,\n",
              "  0.01478601898998022,\n",
              "  0.04861581325531006,\n",
              "  0.003963582217693329,\n",
              "  0.028364384546875954,\n",
              "  0.029293349012732506,\n",
              "  0.026552904397249222,\n",
              "  -0.007582673337310553,\n",
              "  0.01625688001513481,\n",
              "  -0.037777893245220184,\n",
              "  -0.02173777110874653,\n",
              "  0.002655290300026536,\n",
              "  0.011797850020229816,\n",
              "  0.0035087766591459513,\n",
              "  0.017247775569558144,\n",
              "  -0.02850372903048992,\n",
              "  -0.0024249847047030926,\n",
              "  -0.04589084908366203,\n",
              "  0.042980093508958817,\n",
              "  0.025345250964164734,\n",
              "  0.0749984085559845,\n",
              "  0.03684892877936363,\n",
              "  -0.040533822029829025,\n",
              "  -0.005492502823472023,\n",
              "  0.002568199997767806,\n",
              "  -0.01183655671775341,\n",
              "  -0.007516871672123671,\n",
              "  0.014298313297331333,\n",
              "  -0.01061342004686594,\n",
              "  -0.019430842250585556,\n",
              "  -0.0005273809074424207,\n",
              "  -0.07165413349866867,\n",
              "  -0.08391646295785904,\n",
              "  0.0571313202381134,\n",
              "  -0.008252302184700966,\n",
              "  0.07183992862701416,\n",
              "  0.014971812255680561,\n",
              "  0.04093637317419052,\n",
              "  -0.015149863436818123,\n",
              "  0.004068090580403805,\n",
              "  0.0006560812471434474,\n",
              "  -0.04508574679493904,\n",
              "  0.01991080678999424,\n",
              "  0.04796553775668144,\n",
              "  0.01727874018251896,\n",
              "  0.008368422277271748,\n",
              "  -0.00036336062476038933,\n",
              "  0.012920348905026913,\n",
              "  0.02638259343802929,\n",
              "  -0.037592098116874695,\n",
              "  0.008453577756881714,\n",
              "  0.005956985522061586,\n",
              "  -0.06694737821817398,\n",
              "  -0.019678566604852676,\n",
              "  -0.06168324872851372,\n",
              "  0.027683144435286522,\n",
              "  0.022651253268122673,\n",
              "  0.028054730966687202,\n",
              "  -0.0034197508357465267,\n",
              "  -0.013408055528998375,\n",
              "  0.024338871240615845,\n",
              "  0.013872537761926651,\n",
              "  0.0536322221159935,\n",
              "  -0.005674425046890974,\n",
              "  -0.01732518896460533,\n",
              "  -0.03008296899497509,\n",
              "  0.002688191132619977,\n",
              "  0.02556200884282589,\n",
              "  0.0741933062672615,\n",
              "  0.029804280027747154,\n",
              "  0.0010266994358971715,\n",
              "  -0.011650764383375645,\n",
              "  -0.04489995539188385,\n",
              "  0.03156931325793266,\n",
              "  0.028240522369742393,\n",
              "  0.008647112175822258,\n",
              "  0.04183437302708626,\n",
              "  0.020003704354166985,\n",
              "  -0.02568587101995945,\n",
              "  0.012122987769544125,\n",
              "  0.015900777652859688,\n",
              "  0.0016179466620087624,\n",
              "  -0.0632934495806694,\n",
              "  -0.007265277206897736,\n",
              "  0.014422174543142319,\n",
              "  -0.04127699136734009,\n",
              "  -0.0491112619638443,\n",
              "  -0.051402706652879715,\n",
              "  0.027807006612420082,\n",
              "  -0.013980916701257229,\n",
              "  0.0009376736124977469,\n",
              "  0.018610257655382156,\n",
              "  -0.024756906554102898,\n",
              "  -0.03002103790640831,\n",
              "  -0.0307177621871233,\n",
              "  -0.018006429076194763,\n",
              "  -0.0003507809014990926,\n",
              "  -0.01865670457482338,\n",
              "  -0.03805658221244812,\n",
              "  0.053539324551820755,\n",
              "  -0.002372730290517211,\n",
              "  0.025856180116534233,\n",
              "  0.03229700028896332,\n",
              "  0.009320611134171486,\n",
              "  0.0037623064126819372,\n",
              "  -0.01720132678747177,\n",
              "  0.019616635516285896,\n",
              "  -0.016040122136473656,\n",
              "  0.016674913465976715,\n",
              "  -0.01124047115445137,\n",
              "  -0.027714109048247337,\n",
              "  -0.0029397858306765556,\n",
              "  0.015436294488608837,\n",
              "  -0.03235893324017525,\n",
              "  -0.003599737770855427,\n",
              "  0.008376163430511951,\n",
              "  0.06180711090564728,\n",
              "  -0.028178591281175613,\n",
              "  -0.065337173640728,\n",
              "  -0.04514767974615097,\n",
              "  0.005120917223393917,\n",
              "  0.022573839873075485,\n",
              "  -0.03020683117210865,\n",
              "  -0.03365948423743248,\n",
              "  -0.004873193334788084,\n",
              "  -0.010752764530479908,\n",
              "  0.0037506944499909878,\n",
              "  -0.017170362174510956,\n",
              "  0.05332256481051445,\n",
              "  0.02336345985531807,\n",
              "  0.007729759439826012,\n",
              "  0.004598374478518963,\n",
              "  -0.03077969327569008,\n",
              "  -0.019879842177033424,\n",
              "  0.04350650683045387,\n",
              "  -0.0025933594442903996,\n",
              "  -0.019864359870553017,\n",
              "  -0.004451288376003504,\n",
              "  0.011418523266911507,\n",
              "  -0.04081251099705696,\n",
              "  -0.044807057827711105,\n",
              "  0.04567409306764603,\n",
              "  -0.030423589050769806,\n",
              "  0.042670439928770065,\n",
              "  0.019059255719184875,\n",
              "  -0.005879571661353111,\n",
              "  -0.05344642698764801,\n",
              "  0.007257535587996244,\n",
              "  -0.03948099538683891,\n",
              "  -0.06047559529542923,\n",
              "  -0.003212669165804982,\n",
              "  0.013036469928920269,\n",
              "  0.01482472661882639,\n",
              "  0.02831793576478958,\n",
              "  -0.006131166126579046,\n",
              "  0.006615001708269119,\n",
              "  -0.01814577542245388,\n",
              "  -0.03687989339232445,\n",
              "  0.012138470076024532,\n",
              "  0.014747312292456627,\n",
              "  0.013113883323967457,\n",
              "  -0.018254153430461884,\n",
              "  0.005531209986656904,\n",
              "  -0.019802428781986237,\n",
              "  0.0070175533182919025,\n",
              "  -0.0073194666765630245,\n",
              "  -0.07035358250141144,\n",
              "  0.0071723805740475655,\n",
              "  0.021397151052951813,\n",
              "  -0.0088870944455266,\n",
              "  -0.05493277311325073,\n",
              "  -0.014600226655602455,\n",
              "  0.02494269795715809,\n",
              "  0.0028546308167278767,\n",
              "  0.0032900827936828136,\n",
              "  0.0503498800098896,\n",
              "  0.04106023535132408,\n",
              "  -0.031739622354507446,\n",
              "  0.01428283005952835,\n",
              "  -0.0071336738765239716,\n",
              "  0.008469060063362122,\n",
              "  0.022078391164541245,\n",
              "  -0.004784167744219303,\n",
              "  0.032947275787591934,\n",
              "  0.015444035641849041,\n",
              "  -0.014391209930181503,\n",
              "  0.001145722926594317,\n",
              "  -0.044992849230766296,\n",
              "  0.05226973816752434,\n",
              "  0.013632555492222309,\n",
              "  -0.008267784491181374,\n",
              "  0.038273341953754425,\n",
              "  0.026460006833076477,\n",
              "  -0.02097911573946476,\n",
              "  -0.000140796197229065,\n",
              "  0.0020824288949370384,\n",
              "  -0.01625688001513481,\n",
              "  0.037808857858181,\n",
              "  0.009816058911383152,\n",
              "  0.03687989339232445,\n",
              "  -0.007888457737863064,\n",
              "  -0.020174013450741768,\n",
              "  0.006255028303712606,\n",
              "  -0.00317783304490149,\n",
              "  0.013973175548017025,\n",
              "  0.012858417816460133,\n",
              "  0.0073697855696082115,\n",
              "  -0.050411809235811234,\n",
              "  0.048770640045404434,\n",
              "  0.018796049058437347,\n",
              "  -0.008329715579748154,\n",
              "  -0.008376163430511951,\n",
              "  -0.022465460002422333,\n",
              "  0.004718366079032421,\n",
              "  0.031801555305719376,\n",
              "  -0.05245553329586983,\n",
              "  -0.03975968435406685,\n",
              "  -0.01243264228105545,\n",
              "  0.019074739888310432,\n",
              "  -0.018130291253328323,\n",
              "  0.014902140013873577,\n",
              "  0.039419062435626984,\n",
              "  -0.032575689256191254,\n",
              "  -0.03914037346839905,\n",
              "  -0.008438094519078732,\n",
              "  -0.012765521183609962,\n",
              "  -0.01070631667971611,\n",
              "  -0.015567897818982601,\n",
              "  0.010954040102660656,\n",
              "  0.005055115558207035,\n",
              "  -0.01545177772641182,\n",
              "  -0.008894835598766804,\n",
              "  0.02494269795715809,\n",
              "  -0.00894902553409338,\n",
              "  0.019446324557065964,\n",
              "  0.002094040857627988,\n",
              "  0.016427190974354744,\n",
              "  0.022573839873075485,\n",
              "  0.0018279313808307052,\n",
              "  0.024385320022702217,\n",
              "  0.002767540281638503,\n",
              "  -0.012262332253158092,\n",
              "  0.025267835706472397,\n",
              "  0.018703153356909752,\n",
              "  0.008685818873345852,\n",
              "  0.009498662315309048,\n",
              "  -0.040100302547216415,\n",
              "  -0.011039195582270622,\n",
              "  0.0011776561150327325,\n",
              "  -0.03412396460771561,\n",
              "  -0.0008452609763480723,\n",
              "  0.04403292015194893,\n",
              "  0.0012744233245030046,\n",
              "  -0.0009918631985783577,\n",
              "  0.026103904470801353,\n",
              "  0.03201831132173538,\n",
              "  -0.020235944539308548,\n",
              "  0.03768499568104744,\n",
              "  0.008058767765760422,\n",
              "  -0.0033558844588696957,\n",
              "  -0.005910537205636501,\n",
              "  0.04551926255226135,\n",
              "  0.00017865633708424866,\n",
              "  0.002258545020595193,\n",
              "  -0.024958182126283646,\n",
              "  0.018935395404696465,\n",
              "  0.02512849122285843,\n",
              "  -0.0111475745216012,\n",
              "  0.016102053225040436,\n",
              "  -0.005349287763237953,\n",
              "  -0.02323959767818451,\n",
              "  0.02128877118229866,\n",
              "  0.053353533148765564,\n",
              "  0.006347924470901489,\n",
              "  0.009196748957037926,\n",
              "  0.03935713320970535,\n",
              "  -0.004857710562646389,\n",
              "  -0.06440821290016174,\n",
              "  -0.019446324557065964,\n",
              "  0.010868885554373264,\n",
              "  0.016504604369401932,\n",
              "  0.013113883323967457,\n",
              "  -0.004993184469640255,\n",
              "  0.024911733344197273,\n",
              "  0.034062035381793976,\n",
              "  0.02907659113407135,\n",
              "  -0.03070227988064289,\n",
              "  0.04075057804584503,\n",
              "  -0.003955840598791838,\n",
              "  0.016504604369401932,\n",
              "  0.01089210994541645,\n",
              "  0.009947662241756916,\n",
              "  -0.047562986612319946,\n",
              "  0.031244175508618355,\n",
              "  -0.015304691158235073,\n",
              "  0.011534643359482288,\n",
              "  -0.019957255572080612,\n",
              "  -0.017913533374667168,\n",
              "  0.015691759064793587,\n",
              "  -0.01607108674943447,\n",
              "  -0.03511486202478409,\n",
              "  -0.0367869958281517,\n",
              "  -0.01127143669873476,\n",
              "  -0.0014931169571354985,\n",
              "  0.05911311134696007,\n",
              "  0.023208631202578545,\n",
              "  -0.01136433333158493,\n",
              "  -0.0074859061278402805,\n",
              "  0.021118462085723877,\n",
              "  -0.017356155440211296,\n",
              "  -0.022093873471021652,\n",
              "  0.010481816716492176,\n",
              "  0.000688014377374202,\n",
              "  0.018161257728934288,\n",
              "  0.03886168450117111,\n",
              "  0.05000925809144974,\n",
              "  0.010025075636804104,\n",
              "  0.037777893245220184,\n",
              "  0.024137595668435097,\n",
              "  -0.014011882245540619,\n",
              "  -0.042484644800424576,\n",
              "  -0.049389950931072235,\n",
              "  0.04551926255226135,\n",
              "  -0.021397151052951813,\n",
              "  0.015606604516506195,\n",
              "  -0.03384527564048767,\n",
              "  -0.05319870263338089,\n",
              "  0.03734437748789787,\n",
              "  -0.011704953387379646,\n",
              "  0.014011882245540619,\n",
              "  -0.005302839446812868,\n",
              "  0.005345417186617851,\n",
              "  -0.0169536042958498,\n",
              "  -0.018192222341895103,\n",
              "  -0.013547400012612343,\n",
              "  0.017929015681147575,\n",
              "  0.013771899975836277,\n",
              "  0.012347487732768059,\n",
              "  -0.01639622449874878,\n",
              "  0.008538732305169106,\n",
              "  0.01007926557213068,\n",
              "  -0.022279666736721992,\n",
              "  -0.008167146705091,\n",
              "  -0.018378015607595444,\n",
              "  0.0010141197126358747,\n",
              "  -0.00028376965201459825,\n",
              "  -0.027961833402514458,\n",
              "  0.017170362174510956,\n",
              "  -0.004311943892389536,\n",
              "  0.026599351316690445,\n",
              "  0.02669224888086319,\n",
              "  0.01952373795211315,\n",
              "  -0.004602245520800352,\n",
              "  0.02330152876675129,\n",
              "  0.004563538357615471,\n",
              "  -0.0006768861785531044,\n",
              "  0.007857492193579674,\n",
              "  0.018610257655382156,\n",
              "  -0.009661231189966202,\n",
              "  -0.007958129979670048,\n",
              "  -0.06645192950963974,\n",
              "  -0.0010528265265747905,\n",
              "  -0.04332071542739868,\n",
              "  0.02317766658961773,\n",
              "  -0.004869322758167982,\n",
              "  0.01638074219226837,\n",
              "  -0.00910385325551033,\n",
              "  0.003046229714527726,\n",
              "  0.0017979334807023406,\n",
              "  -0.0024133725091814995,\n",
              "  -0.01902829110622406,\n",
              "  0.009281904436647892,\n",
              "  0.025840697810053825,\n",
              "  -0.02876693569123745,\n",
              "  0.0009439634741283953,\n",
              "  0.02285252884030342,\n",
              "  0.0014563455479219556,\n",
              "  -0.010683092288672924,\n",
              "  0.031615760177373886,\n",
              "  -0.0072691477835178375,\n",
              "  0.016659431159496307,\n",
              "  -0.03530065342783928,\n",
              "  -0.005631847772747278,\n",
              "  0.008840646594762802,\n",
              "  -0.03972871974110603,\n",
              "  0.03014490008354187,\n",
              "  0.035393550992012024,\n",
              "  0.019059255719184875,\n",
              "  0.0032765355426818132,\n",
              "  0.0077723367139697075,\n",
              "  -0.02204742655158043,\n",
              "  -0.0003981968038715422,\n",
              "  -0.01933794654905796,\n",
              "  -0.06961040943861008,\n",
              "  0.014460882171988487,\n",
              "  0.016674913465976715,\n",
              "  0.0012831323547288775,\n",
              "  -0.033566586673259735,\n",
              "  -0.029277866706252098,\n",
              "  0.012819711118936539,\n",
              "  -0.015583381056785583,\n",
              "  0.0009662199299782515,\n",
              "  -0.02255835570394993,\n",
              "  0.009351576678454876,\n",
              "  0.0017698709852993488,\n",
              "  0.030005555599927902,\n",
              "  0.015281466767191887,\n",
              "  0.0019140540389344096,\n",
              "  0.019678566604852676,\n",
              "  -0.006177614443004131,\n",
              "  -0.012355228886008263,\n",
              "  0.022759631276130676,\n",
              "  0.012889383360743523,\n",
              "  0.023951802402734756,\n",
              "  0.029742348939180374,\n",
              "  0.02876693569123745,\n",
              "  0.01958566904067993,\n",
              "  0.04802746698260307,\n",
              "  -0.04929705336689949,\n",
              "  -0.002796570537611842,\n",
              "  -0.00554282171651721,\n",
              "  -0.018130291253328323,\n",
              "  -0.011751402169466019,\n",
              "  0.01312162447720766,\n",
              "  0.026041973382234573,\n",
              "  0.02310025319457054,\n",
              "  0.008740007877349854,\n",
              "  -0.0020843641832470894,\n",
              "  0.01902829110622406,\n",
              "  0.047562986612319946,\n",
              "  0.02825600653886795,\n",
              "  0.008910318836569786,\n",
              "  -0.019229566678404808,\n",
              "  -0.02424597553908825,\n",
              "  -0.02385890670120716,\n",
              "  0.0013005504151806235,\n",
              "  0.012254591099917889,\n",
              "  0.022000977769494057,\n",
              "  -0.0067349933087825775,\n",
              "  0.008376163430511951,\n",
              "  -0.11618249863386154,\n",
              "  0.016752326861023903,\n",
              "  0.031182244420051575,\n",
              "  -0.018950877711176872,\n",
              "  -0.0038242375012487173,\n",
              "  0.031925417482852936,\n",
              "  0.025949077680706978,\n",
              "  0.020576564595103264,\n",
              "  -0.008724525570869446,\n",
              "  -0.02831793576478958,\n",
              "  0.024694975465536118,\n",
              "  0.014871174469590187,\n",
              "  0.0007910713902674615,\n",
              "  0.008167146705091,\n",
              "  -0.014422174543142319,\n",
              "  0.020065635442733765,\n",
              "  -0.015459518879652023,\n",
              "  0.029587522149086,\n",
              "  -0.014460882171988487,\n",
              "  2.4720980945858173e-05,\n",
              "  -0.0014960200060158968,\n",
              "  -0.0066382260993123055,\n",
              "  0.017913533374667168,\n",
              "  0.01086114440113306,\n",
              "  -0.022465460002422333,\n",
              "  0.01151916105300188,\n",
              "  -0.031166762113571167,\n",
              "  -0.010164421051740646,\n",
              "  0.013864796608686447,\n",
              "  -0.035393550992012024,\n",
              "  0.029277866706252098,\n",
              "  -0.003615220542997122,\n",
              "  0.002566264709457755,\n",
              "  -0.008623887784779072,\n",
              "  -0.015869811177253723,\n",
              "  0.008205853402614594,\n",
              "  -0.01744905114173889,\n",
              "  -0.02644452452659607,\n",
              "  0.033133070915937424,\n",
              "  0.009219973348081112,\n",
              "  -0.010365696623921394,\n",
              "  -0.0565120130777359,\n",
              "  0.007660087198019028,\n",
              "  0.0070059411227703094,\n",
              "  -0.01997273787856102,\n",
              "  -0.023518286645412445,\n",
              "  -0.0011515290243551135,\n",
              "  -0.027172213420271873,\n",
              "  -0.011913971044123173,\n",
              "  -0.029804280027747154,\n",
              "  0.018176740035414696,\n",
              "  -0.015560156665742397,\n",
              "  0.008894835598766804,\n",
              "  0.020499151200056076,\n",
              "  0.029943624511361122,\n",
              "  0.031058382242918015,\n",
              "  -0.023533768951892853,\n",
              "  -0.00907288771122694,\n",
              "  0.01771225780248642,\n",
              "  0.029123039916157722,\n",
              "  0.01734067127108574,\n",
              "  0.028751453384757042,\n",
              "  -0.0006009239587001503,\n",
              "  -0.03567223995923996,\n",
              "  -0.003067518351599574,\n",
              "  -0.013640296645462513,\n",
              "  -0.019678566604852676,\n",
              "  0.029587522149086,\n",
              "  -0.011681729927659035,\n",
              "  -0.013191296719014645,\n",
              "  0.03694182261824608,\n",
              "  -0.030950002372264862,\n",
              "  0.0069865877740085125,\n",
              "  0.026521937921643257,\n",
              "  -0.0007054324960336089,\n",
              "  -0.024540146812796593,\n",
              "  -0.012192660011351109,\n",
              "  0.02443176880478859,\n",
              "  -0.024849802255630493,\n",
              "  0.0022140322253108025,\n",
              "  0.026212284341454506,\n",
              "  0.03160027787089348,\n",
              "  0.01834705099463463,\n",
              "  0.015250502154231071,\n",
              "  -0.04152471572160721,\n",
              "  -0.022217735648155212,\n",
              "  -0.01136433333158493,\n",
              "  -0.0075207422487437725,\n",
              "  -0.03669410198926926,\n",
              "  0.0019682436250150204,\n",
              "  0.023719562217593193,\n",
              "  0.02059204876422882,\n",
              "  -0.028921764343976974,\n",
              "  -0.021133944392204285,\n",
              "  -0.058803457766771317,\n",
              "  -0.015281466767191887,\n",
              "  0.00513252941891551,\n",
              "  -0.027899902313947678,\n",
              "  0.007513001095503569,\n",
              "  0.016690397635102272,\n",
              "  0.022124839946627617,\n",
              "  0.011913971044123173,\n",
              "  0.023007355630397797,\n",
              "  0.006433079484850168,\n",
              "  -0.04226788878440857,\n",
              "  -0.03523872420191765,\n",
              "  0.011704953387379646,\n",
              "  -0.011464971117675304,\n",
              "  0.011782367713749409,\n",
              "  0.013260968960821629,\n",
              "  0.024462733417749405,\n",
              "  -0.004954477772116661,\n",
              "  0.004788038320839405,\n",
              "  0.032668586820364,\n",
              "  0.011348850093781948,\n",
              "  0.03756113350391388,\n",
              "  0.007447199430316687,\n",
              "  0.00467965891584754,\n",
              "  -0.022759631276130676,\n",
              "  -0.01246360782533884,\n",
              "  -0.01701553352177143,\n",
              "  0.01608656905591488,\n",
              "  0.009336094371974468,\n",
              "  -0.013903503306210041,\n",
              "  -0.03170865774154663,\n",
              "  0.037530168890953064,\n",
              "  0.001053794170729816,\n",
              "  0.051650431007146835,\n",
              "  0.003034617519006133,\n",
              "  -0.0063711488619446754,\n",
              "  0.025778766721487045,\n",
              "  0.003582319710403681,\n",
              "  0.01111660897731781,\n",
              "  -0.0002675611467566341,\n",
              "  0.05022601783275604,\n",
              "  -0.017123913392424583,\n",
              "  -0.005148012191057205,\n",
              "  -0.025763284415006638,\n",
              "  -0.01947729103267193,\n",
              "  -0.02280608005821705,\n",
              "  0.02593359351158142,\n",
              "  -0.04533347114920616,\n",
              "  0.005798287224024534,\n",
              "  0.00783813837915659,\n",
              "  -0.03084162436425686,\n",
              "  -0.01734067127108574,\n",
              "  -0.03774692863225937,\n",
              "  0.06016593798995018,\n",
              "  -0.008709043264389038,\n",
              "  0.005384123884141445,\n",
              "  -0.020143048837780952,\n",
              "  -0.015776915475726128,\n",
              "  0.008941284380853176,\n",
              "  -0.0307177621871233,\n",
              "  0.013826088979840279,\n",
              "  0.017371637746691704,\n",
              "  -0.00402164226397872,\n",
              "  0.013988657854497433,\n",
              "  0.023286044597625732,\n",
              "  0.005767321679741144,\n",
              "  0.02223321795463562,\n",
              "  0.0274354200810194,\n",
              "  -0.01933794654905796,\n",
              "  0.012153953313827515,\n",
              "  0.019988220185041428,\n",
              "  0.04477609321475029,\n",
              "  -0.017696775496006012,\n",
              "  -0.021366184577345848,\n",
              "  -0.0433516800403595,\n",
              "  0.014058330096304417,\n",
              "  -0.014429916627705097,\n",
              "  0.03675603121519089,\n",
              "  0.019446324557065964,\n",
              "  0.0037352116778492928,\n",
              "  -0.00441258167847991,\n",
              "  -0.004149375017732382,\n",
              "  0.011418523266911507,\n",
              "  0.020204979926347733,\n",
              "  -0.01422089897096157,\n",
              "  0.0027907644398510456,\n",
              "  0.010876626707613468,\n",
              "  0.019709531217813492,\n",
              "  -0.022388046607375145,\n",
              "  0.02204742655158043,\n",
              "  0.04350650683045387,\n",
              "  0.012819711118936539,\n",
              "  -0.008399387821555138,\n",
              "  0.023347975686192513,\n",
              "  0.03138351812958717,\n",
              "  0.03449555113911629,\n",
              "  0.0445902980864048,\n",
              "  0.03211120888590813,\n",
              "  -0.030810657888650894,\n",
              "  -0.021706804633140564,\n",
              "  0.015792397782206535,\n",
              "  -0.019817911088466644,\n",
              "  -0.0004891579155810177,\n",
              "  0.0014766666572540998,\n",
              "  -0.0012782939011231065,\n",
              "  0.012378453277051449,\n",
              "  0.008879353292286396,\n",
              "  0.060444626957178116,\n",
              "  0.05186718702316284,\n",
              "  ...],\n",
              " [0.0030098105780780315,\n",
              "  0.029841629788279533,\n",
              "  0.004133774898946285,\n",
              "  -0.001635028631426394,\n",
              "  -0.010047788731753826,\n",
              "  0.0202011838555336,\n",
              "  0.013623353093862534,\n",
              "  0.0044996291399002075,\n",
              "  -0.016127755865454674,\n",
              "  -0.011518748477101326,\n",
              "  0.046074993908405304,\n",
              "  -0.01786273531615734,\n",
              "  -0.05886857584118843,\n",
              "  -0.014641709625720978,\n",
              "  0.023112930357456207,\n",
              "  0.045230135321617126,\n",
              "  -0.014671883545815945,\n",
              "  -0.010636172257363796,\n",
              "  -0.03485043719410896,\n",
              "  0.02480264939367771,\n",
              "  0.0013144348049536347,\n",
              "  0.00024822450359351933,\n",
              "  0.014362604357302189,\n",
              "  0.05554948374629021,\n",
              "  -0.04384215176105499,\n",
              "  -0.012054328806698322,\n",
              "  0.015886368229985237,\n",
              "  0.010341980494558811,\n",
              "  0.03110891953110695,\n",
              "  -0.03657032921910286,\n",
              "  -0.022977149114012718,\n",
              "  -0.05162692442536354,\n",
              "  0.028981683775782585,\n",
              "  -0.018828287720680237,\n",
              "  0.011609269306063652,\n",
              "  -0.008916281163692474,\n",
              "  -0.009187842719256878,\n",
              "  0.01668596640229225,\n",
              "  0.023067669942975044,\n",
              "  -0.018194643780589104,\n",
              "  0.026341499760746956,\n",
              "  -0.06783011555671692,\n",
              "  -0.004609008319675922,\n",
              "  0.012522018514573574,\n",
              "  -0.009670618921518326,\n",
              "  0.02581346221268177,\n",
              "  -0.018511466681957245,\n",
              "  -0.0014455011114478111,\n",
              "  0.023806922137737274,\n",
              "  0.04287659749388695,\n",
              "  0.02109130471944809,\n",
              "  0.011149122379720211,\n",
              "  -0.016323884949088097,\n",
              "  0.02890625037252903,\n",
              "  -0.011963807977735996,\n",
              "  0.00826754979789257,\n",
              "  0.0012258000206202269,\n",
              "  0.07682183384895325,\n",
              "  -0.0014483298873528838,\n",
              "  -0.03795831277966499,\n",
              "  0.06233853101730347,\n",
              "  0.009383970871567726,\n",
              "  -0.001647286699153483,\n",
              "  -0.009172756224870682,\n",
              "  -0.014196650125086308,\n",
              "  -0.007769686169922352,\n",
              "  -0.03412627428770065,\n",
              "  0.0356651246547699,\n",
              "  -0.0039602769538760185,\n",
              "  0.03141065314412117,\n",
              "  0.06306269764900208,\n",
              "  -0.0147473169490695,\n",
              "  -0.07483037561178207,\n",
              "  -0.0047938209027051926,\n",
              "  -0.0064910827204585075,\n",
              "  -0.03412627428770065,\n",
              "  -0.022856455296278,\n",
              "  0.022207723930478096,\n",
              "  0.01136788073927164,\n",
              "  0.03940664231777191,\n",
              "  -0.013253726996481419,\n",
              "  0.014649253338575363,\n",
              "  -0.022901715710759163,\n",
              "  -0.019099850207567215,\n",
              "  -0.01357809267938137,\n",
              "  0.024681955575942993,\n",
              "  -0.08997749537229538,\n",
              "  0.028559254482388496,\n",
              "  -0.014588906429708004,\n",
              "  -0.00040498547605238855,\n",
              "  -0.025421205908060074,\n",
              "  -0.004609008319675922,\n",
              "  -0.023037496954202652,\n",
              "  0.012295717373490334,\n",
              "  0.015524285845458508,\n",
              "  0.007535841315984726,\n",
              "  -0.05901944264769554,\n",
              "  -0.024153918027877808,\n",
              "  -0.009670618921518326,\n",
              "  -0.03491078317165375,\n",
              "  0.012273087166249752,\n",
              "  -0.01217502262443304,\n",
              "  -0.050088074058294296,\n",
              "  0.02003522962331772,\n",
              "  0.03144082799553871,\n",
              "  -0.001518106204457581,\n",
              "  -0.001371953054331243,\n",
              "  0.008335440419614315,\n",
              "  -0.057903021574020386,\n",
              "  -0.035333212465047836,\n",
              "  -0.07265788316726685,\n",
              "  0.03352280333638191,\n",
              "  0.04396284744143486,\n",
              "  0.05600208789110184,\n",
              "  0.02463669516146183,\n",
              "  0.010025158524513245,\n",
              "  0.03412627428770065,\n",
              "  0.013691243715584278,\n",
              "  0.01142068486660719,\n",
              "  -0.007784773129969835,\n",
              "  0.0073170834220945835,\n",
              "  -0.03382453694939613,\n",
              "  -0.024380220100283623,\n",
              "  -0.022524546831846237,\n",
              "  -0.010900191031396389,\n",
              "  -0.0551874041557312,\n",
              "  0.045622389763593674,\n",
              "  -0.05455375835299492,\n",
              "  -0.000950937916059047,\n",
              "  0.03578581660985947,\n",
              "  -0.0005492527270689607,\n",
              "  -0.022539634257555008,\n",
              "  -0.01828516460955143,\n",
              "  -0.037535883486270905,\n",
              "  -0.06073933467268944,\n",
              "  0.010877560824155807,\n",
              "  -0.03638928756117821,\n",
              "  -0.0087729562073946,\n",
              "  -0.027110924944281578,\n",
              "  0.04616551473736763,\n",
              "  -0.019371412694454193,\n",
              "  -0.04794575273990631,\n",
              "  -0.00667966715991497,\n",
              "  -0.0551874041557312,\n",
              "  -0.020683960989117622,\n",
              "  -0.02757861465215683,\n",
              "  0.0008160999277606606,\n",
              "  0.03675137087702751,\n",
              "  -0.0031833082903176546,\n",
              "  -0.016399318352341652,\n",
              "  -0.01860198751091957,\n",
              "  0.05917030945420265,\n",
              "  -0.024983691051602364,\n",
              "  -0.008214745670557022,\n",
              "  -0.04577326029539108,\n",
              "  0.011714876629412174,\n",
              "  0.040945492684841156,\n",
              "  -0.009640445932745934,\n",
              "  -0.016414405778050423,\n",
              "  0.035755645483732224,\n",
              "  -0.05654521286487579,\n",
              "  -0.07833050936460495,\n",
              "  0.022373680025339127,\n",
              "  -0.009248189628124237,\n",
              "  0.010854930616915226,\n",
              "  0.030052844434976578,\n",
              "  0.011315077543258667,\n",
              "  0.010787039995193481,\n",
              "  -0.07332170009613037,\n",
              "  -0.02996232360601425,\n",
              "  -0.048458702862262726,\n",
              "  0.03168221563100815,\n",
              "  -0.028076477348804474,\n",
              "  -0.05536844581365585,\n",
              "  -0.0026156685780733824,\n",
              "  0.008780499920248985,\n",
              "  -0.024968603625893593,\n",
              "  -0.017500652000308037,\n",
              "  -0.022388765588402748,\n",
              "  0.029494633898139,\n",
              "  0.02594924345612526,\n",
              "  0.034458182752132416,\n",
              "  0.046376731246709824,\n",
              "  0.01916019804775715,\n",
              "  -0.020140837877988815,\n",
              "  -0.00697008753195405,\n",
              "  0.026929883286356926,\n",
              "  0.02801613137125969,\n",
              "  0.03657032921910286,\n",
              "  0.007294453214854002,\n",
              "  0.009119952097535133,\n",
              "  -0.009670618921518326,\n",
              "  -0.007973358035087585,\n",
              "  -0.005220022052526474,\n",
              "  -0.007305768318474293,\n",
              "  -0.024953516200184822,\n",
              "  0.08653771132230759,\n",
              "  -0.016173016279935837,\n",
              "  0.004322359338402748,\n",
              "  0.0073510282672941685,\n",
              "  0.027699308469891548,\n",
              "  0.045954298228025436,\n",
              "  0.013608266599476337,\n",
              "  0.024576347321271896,\n",
              "  -0.019869275391101837,\n",
              "  -0.010372154414653778,\n",
              "  -0.055519312620162964,\n",
              "  0.007724426221102476,\n",
              "  -0.010281633585691452,\n",
              "  -0.02656780183315277,\n",
              "  0.030641229823231697,\n",
              "  -0.028287691995501518,\n",
              "  -0.04323868080973625,\n",
              "  -0.009489578194916248,\n",
              "  -0.029705848544836044,\n",
              "  0.012054328806698322,\n",
              "  -0.0318632572889328,\n",
              "  -0.002444056561216712,\n",
              "  0.02317327819764614,\n",
              "  -0.011232099495828152,\n",
              "  0.0430576391518116,\n",
              "  0.04556204378604889,\n",
              "  0.04173000529408455,\n",
              "  0.035031478852033615,\n",
              "  -0.0061063701286911964,\n",
              "  -0.014973619021475315,\n",
              "  0.027488093823194504,\n",
              "  -0.00973096676170826,\n",
              "  -0.00031092888093553483,\n",
              "  -0.004179035313427448,\n",
              "  0.07289926707744598,\n",
              "  -0.0038565555587410927,\n",
              "  -0.014694513753056526,\n",
              "  -0.022313332185149193,\n",
              "  0.02596433088183403,\n",
              "  -0.023867269977927208,\n",
              "  -0.02582854963839054,\n",
              "  0.035906512290239334,\n",
              "  -0.03675137087702751,\n",
              "  0.06119193881750107,\n",
              "  -0.03469957038760185,\n",
              "  0.002736362861469388,\n",
              "  0.06318338960409164,\n",
              "  0.026748841628432274,\n",
              "  0.015124486759305,\n",
              "  -0.04221278056502342,\n",
              "  -0.023067669942975044,\n",
              "  -0.04429475590586662,\n",
              "  0.028815729543566704,\n",
              "  0.03409609943628311,\n",
              "  0.02270558848977089,\n",
              "  0.006834306754171848,\n",
              "  -0.0016953757731243968,\n",
              "  -0.020005056634545326,\n",
              "  -0.06734734028577805,\n",
              "  -0.03877299651503563,\n",
              "  0.01827007718384266,\n",
              "  0.03599703311920166,\n",
              "  0.02979636937379837,\n",
              "  -0.010605999268591404,\n",
              "  0.009014344774186611,\n",
              "  -0.044596489518880844,\n",
              "  -0.07006295770406723,\n",
              "  0.02447074092924595,\n",
              "  -0.03240638226270676,\n",
              "  -0.022524546831846237,\n",
              "  0.0536787249147892,\n",
              "  0.022464198991656303,\n",
              "  0.023354319855570793,\n",
              "  -0.053165774792432785,\n",
              "  -0.029992498457431793,\n",
              "  0.004473227076232433,\n",
              "  0.053920112550258636,\n",
              "  0.019688233733177185,\n",
              "  -0.0834600105881691,\n",
              "  0.015871280804276466,\n",
              "  -0.022765934467315674,\n",
              "  -0.0030173538252711296,\n",
              "  0.0011795967584475875,\n",
              "  0.05060102418065071,\n",
              "  -0.0014162705047056079,\n",
              "  0.017470479011535645,\n",
              "  -0.04100583866238594,\n",
              "  -0.0069851744920015335,\n",
              "  -0.033945232629776,\n",
              "  -0.021151650696992874,\n",
              "  0.004627866670489311,\n",
              "  -0.004642953164875507,\n",
              "  0.07742530107498169,\n",
              "  0.014407864771783352,\n",
              "  0.05026911571621895,\n",
              "  -0.02671866863965988,\n",
              "  0.033100374042987823,\n",
              "  0.0011767679825425148,\n",
              "  -0.032828811556100845,\n",
              "  -0.007075694855302572,\n",
              "  -0.017938168719410896,\n",
              "  0.0020291705150157213,\n",
              "  -0.004390249960124493,\n",
              "  0.026809189468622208,\n",
              "  -0.0015030193608254194,\n",
              "  0.0017189488280564547,\n",
              "  0.031802911311388016,\n",
              "  0.010900191031396389,\n",
              "  0.010440044105052948,\n",
              "  0.058627188205718994,\n",
              "  0.031923603266477585,\n",
              "  0.009044518694281578,\n",
              "  -0.03364349529147148,\n",
              "  0.006830534897744656,\n",
              "  0.012144849635660648,\n",
              "  -0.025843637064099312,\n",
              "  -0.02213229052722454,\n",
              "  -0.028951510787010193,\n",
              "  -0.0489414818584919,\n",
              "  -0.039889417588710785,\n",
              "  0.0442042350769043,\n",
              "  0.0038414685986936092,\n",
              "  -0.018466206267476082,\n",
              "  -0.032828811556100845,\n",
              "  0.046105168759822845,\n",
              "  -0.0060761962085962296,\n",
              "  -0.00973096676170826,\n",
              "  0.004552432801574469,\n",
              "  0.021800382062792778,\n",
              "  0.015154659748077393,\n",
              "  0.0014539874391630292,\n",
              "  0.04897165298461914,\n",
              "  0.01845111884176731,\n",
              "  -0.03792813792824745,\n",
              "  -0.016022149473428726,\n",
              "  -0.03219516575336456,\n",
              "  -0.01976366899907589,\n",
              "  0.00047452605213038623,\n",
              "  0.013932631351053715,\n",
              "  -0.061735060065984726,\n",
              "  -0.014883098192512989,\n",
              "  0.016972616314888,\n",
              "  -0.008094051852822304,\n",
              "  0.009489578194916248,\n",
              "  -0.03334176167845726,\n",
              "  -0.044777531176805496,\n",
              "  0.007860206998884678,\n",
              "  -0.007720654364675283,\n",
              "  -0.009904463775455952,\n",
              "  0.0056650820188224316,\n",
              "  -0.015026422217488289,\n",
              "  -0.0027740797959268093,\n",
              "  -0.007720654364675283,\n",
              "  0.013042512349784374,\n",
              "  -0.01468697004020214,\n",
              "  0.007294453214854002,\n",
              "  0.032798636704683304,\n",
              "  -0.05002772808074951,\n",
              "  0.008923823945224285,\n",
              "  -0.051415711641311646,\n",
              "  -0.0024987461511045694,\n",
              "  -0.02932867966592312,\n",
              "  0.035755645483732224,\n",
              "  0.030203713104128838,\n",
              "  0.007004032842814922,\n",
              "  0.026733756065368652,\n",
              "  0.05497618764638901,\n",
              "  0.031048571690917015,\n",
              "  0.00431481609120965,\n",
              "  -0.010900191031396389,\n",
              "  -0.05162692442536354,\n",
              "  0.021921075880527496,\n",
              "  0.005631136707961559,\n",
              "  0.011895917356014252,\n",
              "  -0.005891383625566959,\n",
              "  0.0019480790942907333,\n",
              "  -0.018089037388563156,\n",
              "  0.029389027506113052,\n",
              "  -0.04366111010313034,\n",
              "  0.04655776917934418,\n",
              "  0.004360076505690813,\n",
              "  0.052139874547719955,\n",
              "  -0.006249694153666496,\n",
              "  -0.011269817128777504,\n",
              "  0.01349511556327343,\n",
              "  0.01578076183795929,\n",
              "  -0.03883334621787071,\n",
              "  -0.003000381402671337,\n",
              "  0.021423213183879852,\n",
              "  0.0002727404935285449,\n",
              "  -0.020729221403598785,\n",
              "  0.02450091391801834,\n",
              "  -0.03816952556371689,\n",
              "  0.004492085427045822,\n",
              "  0.05105362832546234,\n",
              "  -0.01741013117134571,\n",
              "  0.021030956879258156,\n",
              "  0.01903950236737728,\n",
              "  0.02225298434495926,\n",
              "  0.0055745611898601055,\n",
              "  0.01695752888917923,\n",
              "  0.025768201798200607,\n",
              "  0.01665579341351986,\n",
              "  0.010636172257363796,\n",
              "  0.0675283819437027,\n",
              "  -0.0023667369969189167,\n",
              "  0.004918287042528391,\n",
              "  0.022011596709489822,\n",
              "  -0.0622178390622139,\n",
              "  0.027367400005459785,\n",
              "  -0.024153918027877808,\n",
              "  -0.007079466711729765,\n",
              "  0.03403575345873833,\n",
              "  -0.024953516200184822,\n",
              "  0.002093289280310273,\n",
              "  -0.05413132905960083,\n",
              "  0.03439783304929733,\n",
              "  0.00018811316112987697,\n",
              "  0.011043515056371689,\n",
              "  0.003015468129888177,\n",
              "  0.04160930961370468,\n",
              "  0.03632894158363342,\n",
              "  0.018405858427286148,\n",
              "  0.03967820480465889,\n",
              "  0.004348761402070522,\n",
              "  -0.020321877673268318,\n",
              "  -0.01932615227997303,\n",
              "  -0.01976366899907589,\n",
              "  0.059532392770051956,\n",
              "  0.04794575273990631,\n",
              "  0.024440566077828407,\n",
              "  -0.02374657616019249,\n",
              "  0.004220523871481419,\n",
              "  -0.04900182783603668,\n",
              "  -0.03189343214035034,\n",
              "  0.00022818738943897188,\n",
              "  0.016459666192531586,\n",
              "  0.011028428561985493,\n",
              "  0.02033696509897709,\n",
              "  -0.027669135481119156,\n",
              "  0.009368883445858955,\n",
              "  0.024123745039105415,\n",
              "  -0.01378176361322403,\n",
              "  -0.06547658145427704,\n",
              "  0.027774741873145103,\n",
              "  0.015018879435956478,\n",
              "  -0.03482026234269142,\n",
              "  -0.027548441663384438,\n",
              "  -0.06879566609859467,\n",
              "  0.07289926707744598,\n",
              "  0.004507172387093306,\n",
              "  0.027925610542297363,\n",
              "  0.025149645283818245,\n",
              "  -0.03067140281200409,\n",
              "  -0.04067393019795418,\n",
              "  -0.022494373843073845,\n",
              "  -0.016701053828001022,\n",
              "  0.0049032000824809074,\n",
              "  -0.03439783304929733,\n",
              "  -0.010688976384699345,\n",
              "  0.03877299651503563,\n",
              "  -0.002687330823391676,\n",
              "  0.017183830961585045,\n",
              "  0.009368883445858955,\n",
              "  0.010138309560716152,\n",
              "  -0.016112670302391052,\n",
              "  -0.03307019919157028,\n",
              "  -0.0004813622508663684,\n",
              "  -0.031018398702144623,\n",
              "  0.0008957769605331123,\n",
              "  -0.015230094082653522,\n",
              "  0.06089020147919655,\n",
              "  0.020744308829307556,\n",
              "  0.00046273949556052685,\n",
              "  -0.0007038920884951949,\n",
              "  0.0010079847415909171,\n",
              "  -0.012989708222448826,\n",
              "  0.019507193937897682,\n",
              "  -0.03361332416534424,\n",
              "  -0.0366005040705204,\n",
              "  -0.02388235554099083,\n",
              "  0.0071549005806446075,\n",
              "  -0.006955000571906567,\n",
              "  0.003185194218531251,\n",
              "  -0.02358062006533146,\n",
              "  0.0048767984844744205,\n",
              "  -0.019099850207567215,\n",
              "  0.03065631538629532,\n",
              "  -0.0014888755977153778,\n",
              "  0.017938168719410896,\n",
              "  0.014634166844189167,\n",
              "  0.02890625037252903,\n",
              "  -0.0062760962173342705,\n",
              "  -0.021211998537182808,\n",
              "  -0.02081974223256111,\n",
              "  0.03720397502183914,\n",
              "  0.018964068964123726,\n",
              "  -0.01885846257209778,\n",
              "  -0.022781021893024445,\n",
              "  0.009119952097535133,\n",
              "  0.003747176378965378,\n",
              "  -0.07633905112743378,\n",
              "  0.01150366198271513,\n",
              "  -0.031018398702144623,\n",
              "  0.03780744597315788,\n",
              "  0.02608502469956875,\n",
              "  0.016580360010266304,\n",
              "  -0.004058341030031443,\n",
              "  0.05247178301215172,\n",
              "  -0.012582365423440933,\n",
              "  -0.04390249773859978,\n",
              "  0.004650496877729893,\n",
              "  0.008524024859070778,\n",
              "  0.01827007718384266,\n",
              "  0.040764451026916504,\n",
              "  -0.003383208066225052,\n",
              "  0.009587641805410385,\n",
              "  -0.03159169480204582,\n",
              "  -0.03213481977581978,\n",
              "  0.028257519006729126,\n",
              "  0.030430013313889503,\n",
              "  -0.014852924272418022,\n",
              "  0.021574079990386963,\n",
              "  -0.031802911311388016,\n",
              "  -0.0017830675933510065,\n",
              "  0.01724417693912983,\n",
              "  0.018798114731907845,\n",
              "  -0.054795145988464355,\n",
              "  -0.005167218390852213,\n",
              "  0.008999258279800415,\n",
              "  -0.011375424452126026,\n",
              "  0.0009862976148724556,\n",
              "  -0.012944448739290237,\n",
              "  0.004944688640534878,\n",
              "  0.016414405778050423,\n",
              "  -0.005623593460768461,\n",
              "  0.047704365104436874,\n",
              "  -0.00906714890152216,\n",
              "  -0.014113673008978367,\n",
              "  0.0008858762448653579,\n",
              "  -0.004767419304698706,\n",
              "  -0.011752593331038952,\n",
              "  0.0019688233733177185,\n",
              "  -0.00942923128604889,\n",
              "  0.0275937020778656,\n",
              "  0.024380220100283623,\n",
              "  -0.002493088599294424,\n",
              "  -0.007799860090017319,\n",
              "  -0.05491584166884422,\n",
              "  0.04776471108198166,\n",
              "  -0.006498625967651606,\n",
              "  0.0008321296190842986,\n",
              "  0.01608249545097351,\n",
              "  0.03964802995324135,\n",
              "  -0.0014898184454068542,\n",
              "  -0.008071421645581722,\n",
              "  0.027246706187725067,\n",
              "  -0.005204935558140278,\n",
              "  -0.007577329874038696,\n",
              "  -0.005193620454519987,\n",
              "  0.02269050106406212,\n",
              "  0.010062875226140022,\n",
              "  -0.014181563630700111,\n",
              "  -0.005623593460768461,\n",
              "  0.010055331513285637,\n",
              "  0.020970609039068222,\n",
              "  0.008501394651830196,\n",
              "  0.004450596868991852,\n",
              "  -0.03098822385072708,\n",
              "  0.02641693316400051,\n",
              "  0.014943445101380348,\n",
              "  0.015305527485907078,\n",
              "  0.011307533830404282,\n",
              "  0.01757608726620674,\n",
              "  0.02756352722644806,\n",
              "  0.022343505173921585,\n",
              "  -0.04073427617549896,\n",
              "  0.001387982745654881,\n",
              "  -0.0022611296735703945,\n",
              "  -0.016399318352341652,\n",
              "  -0.008448590524494648,\n",
              "  0.006166717037558556,\n",
              "  0.02537594549357891,\n",
              "  0.0005115357926115394,\n",
              "  -0.04221278056502342,\n",
              "  -0.0023252484388649464,\n",
              "  0.0098139438778162,\n",
              "  0.039919592440128326,\n",
              "  -0.007320854812860489,\n",
              "  0.03379436209797859,\n",
              "  0.03228568658232689,\n",
              "  -0.018979156389832497,\n",
              "  0.0011069916654378176,\n",
              "  0.0196278877556324,\n",
              "  0.006860708352178335,\n",
              "  -0.012114675715565681,\n",
              "  0.00079724146053195,\n",
              "  0.04510943964123726,\n",
              "  0.008976628072559834,\n",
              "  0.001149423304013908,\n",
              "  0.053165774792432785,\n",
              "  -0.035755645483732224,\n",
              "  -0.029826544225215912,\n",
              "  0.03249690309166908,\n",
              "  0.009625359438359737,\n",
              "  0.0002128648920916021,\n",
              "  0.006370388437062502,\n",
              "  -0.03801865875720978,\n",
              "  -0.00285705691203475,\n",
              "  0.0064458223059773445,\n",
              "  -0.01977875456213951,\n",
              "  -0.0039527337066829205,\n",
              "  0.02122708410024643,\n",
              "  0.016731226816773415,\n",
              "  -0.032677941024303436,\n",
              "  0.016459666192531586,\n",
              "  0.04393267259001732,\n",
              "  -0.008742783218622208,\n",
              "  0.01974858157336712,\n",
              "  0.01636914536356926,\n",
              "  -0.005110643338412046,\n",
              "  -0.03696258366107941,\n",
              "  0.052441611886024475,\n",
              "  -0.03551425412297249,\n",
              "  0.0075660147704184055,\n",
              "  -0.006543886382132769,\n",
              "  0.041669659316539764,\n",
              "  -0.004756104201078415,\n",
              "  -0.04441544786095619,\n",
              "  0.014407864771783352,\n",
              "  0.00285705691203475,\n",
              "  -0.03258742392063141,\n",
              "  -0.0079658143222332,\n",
              "  0.012620083056390285,\n",
              "  0.014588906429708004,\n",
              "  0.02196633629500866,\n",
              "  0.04719141498208046,\n",
              "  -0.007830033078789711,\n",
              "  -0.05651503801345825,\n",
              "  -0.01077949721366167,\n",
              "  0.05289421230554581,\n",
              "  0.046105168759822845,\n",
              "  -0.015297984704375267,\n",
              "  -0.00871260929852724,\n",
              "  0.013811937533318996,\n",
              "  0.0032757148146629333,\n",
              "  0.027699308469891548,\n",
              "  -0.03678154572844505,\n",
              "  0.019944708794355392,\n",
              "  -0.013698786497116089,\n",
              "  0.011654529720544815,\n",
              "  0.010741779580712318,\n",
              "  0.00973096676170826,\n",
              "  -0.03291933238506317,\n",
              "  0.010968081653118134,\n",
              "  -0.009474491700530052,\n",
              "  0.016142843291163445,\n",
              "  0.003209710121154785,\n",
              "  -0.006174260284751654,\n",
              "  -0.00401308061555028,\n",
              "  -0.023082757368683815,\n",
              "  -0.03536338731646538,\n",
              "  -0.013932631351053715,\n",
              "  -0.030761923640966415,\n",
              "  -0.020683960989117622,\n",
              "  0.030354579910635948,\n",
              "  0.0073019964620471,\n",
              "  -0.02036713808774948,\n",
              "  -0.024395305663347244,\n",
              "  0.005435008555650711,\n",
              "  -0.0010871903505176306,\n",
              "  -0.00431481609120965,\n",
              "  0.011111405678093433,\n",
              "  0.019280891865491867,\n",
              "  0.004016852471977472,\n",
              "  0.03578581660985947,\n",
              "  0.05717885494232178,\n",
              "  -0.004635409917682409,\n",
              "  0.011186840012669563,\n",
              "  0.03466939553618431,\n",
              "  -0.020382225513458252,\n",
              "  -0.06469206511974335,\n",
              "  -0.029766196385025978,\n",
              "  0.02685444988310337,\n",
              "  0.011941177770495415,\n",
              "  0.007671622093766928,\n",
              "  0.0007802688633091748,\n",
              "  -0.01134525053203106,\n",
              "  0.034458182752132416,\n",
              "  -0.01200906839221716,\n",
              "  0.04043254256248474,\n",
              "  -0.0065702879801392555,\n",
              "  -0.010047788731753826,\n",
              "  -0.03940664231777191,\n",
              "  -0.022328419610857964,\n",
              "  -0.008342983201146126,\n",
              "  0.014294713735580444,\n",
              "  -0.0003917845606338233,\n",
              "  0.031229613348841667,\n",
              "  -0.019522279500961304,\n",
              "  0.02183055505156517,\n",
              "  -0.0011107634054496884,\n",
              "  -0.00741514703258872,\n",
              "  0.006147858686745167,\n",
              "  -0.04317833483219147,\n",
              "  -0.011111405678093433,\n",
              "  -0.005880068521946669,\n",
              "  -0.014030695892870426,\n",
              "  -0.00685693696141243,\n",
              "  -0.004209208767861128,\n",
              "  -0.006494854111224413,\n",
              "  0.031501173973083496,\n",
              "  0.034729745239019394,\n",
              "  -0.00893136765807867,\n",
              "  0.024742301553487778,\n",
              "  -0.016022149473428726,\n",
              "  -0.014324887655675411,\n",
              "  0.0017151770880445838,\n",
              "  -0.0027853948995471,\n",
              "  0.017817474901676178,\n",
              "  -0.015003792010247707,\n",
              "  -0.05886857584118843,\n",
              "  -0.0061063701286911964,\n",
              "  -0.008312810212373734,\n",
              "  -0.016173016279935837,\n",
              "  -0.004163948353379965,\n",
              "  0.02359570749104023,\n",
              "  -0.028845902532339096,\n",
              "  0.019944708794355392,\n",
              "  0.004963546991348267,\n",
              "  0.01466433983296156,\n",
              "  -0.038561783730983734,\n",
              "  0.02772948332130909,\n",
              "  0.011760137043893337,\n",
              "  0.008644718676805496,\n",
              "  -0.00493714539334178,\n",
              "  0.008584371767938137,\n",
              "  -0.0006482596509158611,\n",
              "  0.02388235554099083,\n",
              "  0.023942703381180763,\n",
              "  -0.01650492660701275,\n",
              "  0.024123745039105415,\n",
              "  -0.02685444988310337,\n",
              "  0.000162889962666668,\n",
              "  0.009783769957721233,\n",
              "  -0.029690762981772423,\n",
              "  0.024274611845612526,\n",
              "  0.012861470691859722,\n",
              "  0.01888863556087017,\n",
              "  -0.023399580270051956,\n",
              "  -0.010590911842882633,\n",
              "  -0.011714876629412174,\n",
              "  0.023112930357456207,\n",
              "  0.003139933804050088,\n",
              "  -0.04996738210320473,\n",
              "  0.013442311435937881,\n",
              "  0.005487812217324972,\n",
              "  -0.005476497579365969,\n",
              "  -0.014566276222467422,\n",
              "  -0.011918547563254833,\n",
              "  0.0119487214833498,\n",
              "  0.015735501423478127,\n",
              "  0.014324887655675411,\n",
              "  -0.021347779780626297,\n",
              "  0.01814938336610794,\n",
              "  0.005699027329683304,\n",
              "  -0.0071813021786510944,\n",
              "  0.027473008260130882,\n",
              "  -0.0241840910166502,\n",
              "  -0.000180098315468058,\n",
              "  -0.008448590524494648,\n",
              "  -0.006792818196117878,\n",
              "  -0.0036774000618606806,\n",
              "  0.00878804363310337,\n",
              "  0.012280630879104137,\n",
              "  -0.004375163000077009,\n",
              "  0.03759622946381569,\n",
              "  0.0033586921636015177,\n",
              "  0.06511449813842773,\n",
              "  -0.043449897319078445,\n",
              "  -0.017304524779319763,\n",
              "  0.025858722627162933,\n",
              "  0.011850657872855663,\n",
              "  0.007943184114992619,\n",
              "  0.029494633898139,\n",
              "  0.020608527585864067,\n",
              "  -0.007464179303497076,\n",
              "  0.025028951466083527,\n",
              "  0.0071549005806446075,\n",
              "  -0.03316072002053261,\n",
              "  0.023852182552218437,\n",
              "  0.050088074058294296,\n",
              "  0.010070418938994408,\n",
              "  -0.0075622433796525,\n",
              "  -0.025768201798200607,\n",
              "  -0.013849654234945774,\n",
              "  0.0060233925469219685,\n",
              "  -0.020155923441052437,\n",
              "  -0.0018519009463489056,\n",
              "  0.006479767616838217,\n",
              "  0.013381964527070522,\n",
              "  -0.11296973377466202,\n",
              "  -0.008380700834095478,\n",
              "  0.016293711960315704,\n",
              "  0.007200160995125771,\n",
              "  -0.012144849635660648,\n",
              "  0.04130757600069046,\n",
              "  0.01591654121875763,\n",
              "  0.023565534502267838,\n",
              "  0.00468821357935667,\n",
              "  -0.03373401612043381,\n",
              "  0.027171272784471512,\n",
              "  0.004099829588085413,\n",
              "  -0.010138309560716152,\n",
              "  -0.00871260929852724,\n",
              "  -0.009021888487040997,\n",
              "  0.032979678362607956,\n",
              "  -0.015554459765553474,\n",
              "  0.04055323824286461,\n",
              "  -0.02155899442732334,\n",
              "  -0.012808667495846748,\n",
              "  -0.027518266811966896,\n",
              "  0.04390249773859978,\n",
              "  0.0010060989297926426,\n",
              "  0.006057337857782841,\n",
              "  -0.025692768394947052,\n",
              "  0.010658802464604378,\n",
              "  -0.04619568958878517,\n",
              "  -0.0154111348092556,\n",
              "  0.031621869653463364,\n",
              "  -0.02789543755352497,\n",
              "  0.05032946169376373,\n",
              "  -0.008780499920248985,\n",
              "  0.0018867891049012542,\n",
              "  -0.026371672749519348,\n",
              "  -0.0027099610306322575,\n",
              "  -0.013200923800468445,\n",
              "  -0.020699048414826393,\n",
              "  -0.01535078790038824,\n",
              "  0.04393267259001732,\n",
              "  0.0043299030512571335,\n",
              "  0.00854665506631136,\n",
              "  -0.03789796680212021,\n",
              "  -0.030626142397522926,\n",
              "  0.02641693316400051,\n",
              "  -0.003164449939504266,\n",
              "  0.01594671607017517,\n",
              "  0.0013134918408468366,\n",
              "  -0.03506165370345116,\n",
              "  -0.014113673008978367,\n",
              "  -0.005363346543163061,\n",
              "  0.020261531695723534,\n",
              "  0.009889377281069756,\n",
              "  0.02861960232257843,\n",
              "  0.0025364630855619907,\n",
              "  0.007754599675536156,\n",
              "  0.03844108805060387,\n",
              "  -0.009406601078808308,\n",
              "  -0.03638928756117821,\n",
              "  0.022041769698262215,\n",
              "  0.03877299651503563,\n",
              "  0.0079658143222332,\n",
              "  -0.003251198912039399,\n",
              "  -0.024093570187687874,\n",
              "  -0.015056596137583256,\n",
              "  0.0008505166624672711,\n",
              "  -0.006298726424574852,\n",
              "  -0.0392557755112648,\n",
              "  0.019839102402329445,\n",
              "  -0.009647988714277744,\n",
              "  -0.0120920455083251,\n",
              "  0.01916019804775715,\n",
              "  -0.017048049718141556,\n",
              "  -0.0010815327987074852,\n",
              "  -0.01274832058697939,\n",
              "  0.007053064648061991,\n",
              "  -0.03285898268222809,\n",
              "  0.00420166552066803,\n",
              "  0.009127495810389519,\n",
              "  -0.012439041398465633,\n",
              "  -0.016142843291163445,\n",
              "  0.021181823685765266,\n",
              "  0.04266538470983505,\n",
              "  0.0019820244051516056,\n",
              "  -0.012853927910327911,\n",
              "  -0.005985675845295191,\n",
              "  -0.010568281635642052,\n",
              "  0.0017868392169475555,\n",
              "  0.005921557080000639,\n",
              "  -0.0350918248295784,\n",
              "  0.01828516460955143,\n",
              "  0.05609260872006416,\n",
              "  0.01860198751091957,\n",
              "  -0.01989944837987423,\n",
              "  -0.021649513393640518,\n",
              "  -0.04866991937160492,\n",
              "  -0.012099589221179485,\n",
              "  -0.0017943826969712973,\n",
              "  -0.03008301928639412,\n",
              "  0.000696348724886775,\n",
              "  0.009806400164961815,\n",
              "  0.03023388609290123,\n",
              "  -0.006577831692993641,\n",
              "  0.011616813018918037,\n",
              "  -0.01741013117134571,\n",
              "  -0.037113454192876816,\n",
              "  -0.025451380759477615,\n",
              "  0.0053595746867358685,\n",
              "  0.0185869000852108,\n",
              "  0.01624845154583454,\n",
              "  -0.023942703381180763,\n",
              "  0.009685706347227097,\n",
              "  0.01018356904387474,\n",
              "  0.010138309560716152,\n",
              "  0.05041998252272606,\n",
              "  0.02534577250480652,\n",
              "  0.029283419251441956,\n",
              "  0.018979156389832497,\n",
              "  -0.010010072030127048,\n",
              "  -0.020321877673268318,\n",
              "  -0.03792813792824745,\n",
              "  -0.005740515887737274,\n",
              "  0.032376207411289215,\n",
              "  0.006966315675526857,\n",
              "  0.007335941772907972,\n",
              "  -0.026009591296315193,\n",
              "  0.026582887396216393,\n",
              "  -0.031199440360069275,\n",
              "  0.032828811556100845,\n",
              "  -0.010794583708047867,\n",
              "  0.029856717213988304,\n",
              "  0.009670618921518326,\n",
              "  0.004952231887727976,\n",
              "  0.02462160773575306,\n",
              "  0.030475273728370667,\n",
              "  0.026658322662115097,\n",
              "  -0.021860729902982712,\n",
              "  -0.0023497643414884806,\n",
              "  0.005491584073752165,\n",
              "  0.019280891865491867,\n",
              "  -0.018798114731907845,\n",
              "  0.029238158836960793,\n",
              "  -0.030580881983041763,\n",
              "  0.022041769698262215,\n",
              "  0.0268393624573946,\n",
              "  -0.013442311435937881,\n",
              "  -0.03110891953110695,\n",
              "  -0.01556954625993967,\n",
              "  0.04821731522679329,\n",
              "  -0.02269050106406212,\n",
              "  0.0175459124147892,\n",
              "  -0.0037302037235349417,\n",
              "  -0.00405456917360425,\n",
              "  0.009821486659348011,\n",
              "  -0.010885104537010193,\n",
              "  -0.008757869713008404,\n",
              "  0.010025158524513245,\n",
              "  -0.02136286534368992,\n",
              "  0.02846873365342617,\n",
              "  0.024697041139006615,\n",
              "  0.03943681716918945,\n",
              "  0.005778232589364052,\n",
              "  0.04115670919418335,\n",
              "  -0.02050291933119297,\n",
              "  -0.01274832058697939,\n",
              "  0.012884100899100304,\n",
              "  0.017787301912903786,\n",
              "  -0.024847909808158875,\n",
              "  -0.013283900916576385,\n",
              "  -0.0010956766782328486,\n",
              "  0.008569285273551941,\n",
              "  -0.022162465378642082,\n",
              "  0.04112653434276581,\n",
              "  0.0046467250213027,\n",
              "  0.022358592599630356,\n",
              "  -0.016399318352341652,\n",
              "  -0.002691102446988225,\n",
              "  0.016173016279935837,\n",
              "  -0.03009810484945774,\n",
              "  -0.0008109138580039144,\n",
              "  0.03807900473475456,\n",
              "  -0.0072265625931322575,\n",
              "  -0.002170609077438712,\n",
              "  -0.02875538356602192,\n",
              "  0.04791558161377907,\n",
              "  0.03895403817296028,\n",
              "  -0.0029155181255191565,\n",
              "  0.023716401308774948,\n",
              "  0.003064500167965889,\n",
              "  0.008425961248576641,\n",
              "  0.059653088450431824,\n",
              "  0.02475738897919655,\n",
              "  0.012929361313581467,\n",
              "  -0.03448835387825966,\n",
              "  -0.00468821357935667,\n",
              "  -0.002457257593050599,\n",
              "  -0.007686709053814411,\n",
              "  -0.001558651914820075,\n",
              "  -0.00024233123986050487,\n",
              "  -0.0021479788701981306,\n",
              "  0.016338970512151718,\n",
              "  -0.022750848904252052,\n",
              "  0.030641229823231697,\n",
              "  0.04061358422040939,\n",
              "  ...]]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"infoslack/mistral-7b-arxiv-paper-chunked\", split=\"train\")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gkXIO896glY",
        "outputId": "838ef4ed-40fd-4a36-98c2-fc0a31c3bbb6"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
              "    num_rows: 25\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu1sVlyo6gjC",
        "outputId": "530b8e23-6635-409c-be96-a5fbf4083efd"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'doi': '2310.06825',\n",
              " 'chunk-id': '0',\n",
              " 'chunk': 'Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src',\n",
              " 'id': '2310.06825',\n",
              " 'title': 'Mistral 7B',\n",
              " 'summary': 'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released under the Apache 2.0 license.',\n",
              " 'source': 'http://arxiv.org/pdf/2310.06825',\n",
              " 'authors': ['Albert Q. Jiang',\n",
              "  'Alexandre Sablayrolles',\n",
              "  'Arthur Mensch',\n",
              "  'Chris Bamford',\n",
              "  'Devendra Singh Chaplot',\n",
              "  'Diego de las Casas',\n",
              "  'Florian Bressand',\n",
              "  'Gianna Lengyel',\n",
              "  'Guillaume Lample',\n",
              "  'Lucile Saulnier',\n",
              "  'Lélio Renard Lavaud',\n",
              "  'Marie-Anne Lachaux',\n",
              "  'Pierre Stock',\n",
              "  'Teven Le Scao',\n",
              "  'Thibaut Lavril',\n",
              "  'Thomas Wang',\n",
              "  'Timothée Lacroix',\n",
              "  'William El Sayed'],\n",
              " 'categories': ['cs.CL', 'cs.AI', 'cs.LG'],\n",
              " 'comment': 'Models and code are available at\\n  https://mistral.ai/news/announcing-mistral-7b/',\n",
              " 'journal_ref': None,\n",
              " 'primary_category': 'cs.CL',\n",
              " 'published': '20231010',\n",
              " 'updated': '20231010',\n",
              " 'references': [{'id': '1808.07036'},\n",
              "  {'id': '1809.02789'},\n",
              "  {'id': '1904.10509'},\n",
              "  {'id': '2302.13971'},\n",
              "  {'id': '2009.03300'},\n",
              "  {'id': '2305.13245'},\n",
              "  {'id': '1904.09728'},\n",
              "  {'id': '1803.05457'},\n",
              "  {'id': '2103.03874'},\n",
              "  {'id': '1905.07830'},\n",
              "  {'id': '2308.12950'},\n",
              "  {'id': '2210.09261'},\n",
              "  {'id': '2310.06825'},\n",
              "  {'id': '2307.09288'},\n",
              "  {'id': '2304.06364'},\n",
              "  {'id': '1905.10044'},\n",
              "  {'id': '2110.14168'},\n",
              "  {'id': '2108.07732'},\n",
              "  {'id': '2107.03374'},\n",
              "  {'id': '1811.00937'},\n",
              "  {'id': '2004.05150'},\n",
              "  {'id': '1705.03551'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset.to_pandas()"
      ],
      "metadata": {
        "id": "XAAn9asa6ggs"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "WCazfJsY6geE",
        "outputId": "c71d28eb-1782-4f22-ab71-99934c40ffc3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          doi chunk-id                                              chunk  \\\n",
              "0  2310.06825        0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n",
              "1  2310.06825        1  automated benchmarks. Our models are released ...   \n",
              "2  2310.06825        2  GQA significantly accelerates the inference sp...   \n",
              "3  2310.06825        3  Mistral 7B takes a significant step in balanci...   \n",
              "4  2310.06825        4  parameters of the architecture are summarized ...   \n",
              "\n",
              "           id       title                                            summary  \\\n",
              "0  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
              "1  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
              "2  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
              "3  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
              "4  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
              "\n",
              "                            source  \\\n",
              "0  http://arxiv.org/pdf/2310.06825   \n",
              "1  http://arxiv.org/pdf/2310.06825   \n",
              "2  http://arxiv.org/pdf/2310.06825   \n",
              "3  http://arxiv.org/pdf/2310.06825   \n",
              "4  http://arxiv.org/pdf/2310.06825   \n",
              "\n",
              "                                             authors             categories  \\\n",
              "0  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
              "1  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
              "2  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
              "3  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
              "4  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
              "\n",
              "                                             comment journal_ref  \\\n",
              "0  Models and code are available at\\n  https://mi...        None   \n",
              "1  Models and code are available at\\n  https://mi...        None   \n",
              "2  Models and code are available at\\n  https://mi...        None   \n",
              "3  Models and code are available at\\n  https://mi...        None   \n",
              "4  Models and code are available at\\n  https://mi...        None   \n",
              "\n",
              "  primary_category published   updated  \\\n",
              "0            cs.CL  20231010  20231010   \n",
              "1            cs.CL  20231010  20231010   \n",
              "2            cs.CL  20231010  20231010   \n",
              "3            cs.CL  20231010  20231010   \n",
              "4            cs.CL  20231010  20231010   \n",
              "\n",
              "                                          references  \n",
              "0  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
              "1  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
              "2  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
              "3  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
              "4  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d267c8b-e3f4-4bd5-ada5-51f122b0c769\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doi</th>\n",
              "      <th>chunk-id</th>\n",
              "      <th>chunk</th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>source</th>\n",
              "      <th>authors</th>\n",
              "      <th>categories</th>\n",
              "      <th>comment</th>\n",
              "      <th>journal_ref</th>\n",
              "      <th>primary_category</th>\n",
              "      <th>published</th>\n",
              "      <th>updated</th>\n",
              "      <th>references</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2310.06825</td>\n",
              "      <td>0</td>\n",
              "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
              "      <td>2310.06825</td>\n",
              "      <td>Mistral 7B</td>\n",
              "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
              "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
              "      <td>Models and code are available at\\n  https://mi...</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.CL</td>\n",
              "      <td>20231010</td>\n",
              "      <td>20231010</td>\n",
              "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2310.06825</td>\n",
              "      <td>1</td>\n",
              "      <td>automated benchmarks. Our models are released ...</td>\n",
              "      <td>2310.06825</td>\n",
              "      <td>Mistral 7B</td>\n",
              "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
              "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
              "      <td>Models and code are available at\\n  https://mi...</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.CL</td>\n",
              "      <td>20231010</td>\n",
              "      <td>20231010</td>\n",
              "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2310.06825</td>\n",
              "      <td>2</td>\n",
              "      <td>GQA significantly accelerates the inference sp...</td>\n",
              "      <td>2310.06825</td>\n",
              "      <td>Mistral 7B</td>\n",
              "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
              "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
              "      <td>Models and code are available at\\n  https://mi...</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.CL</td>\n",
              "      <td>20231010</td>\n",
              "      <td>20231010</td>\n",
              "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2310.06825</td>\n",
              "      <td>3</td>\n",
              "      <td>Mistral 7B takes a significant step in balanci...</td>\n",
              "      <td>2310.06825</td>\n",
              "      <td>Mistral 7B</td>\n",
              "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
              "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
              "      <td>Models and code are available at\\n  https://mi...</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.CL</td>\n",
              "      <td>20231010</td>\n",
              "      <td>20231010</td>\n",
              "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2310.06825</td>\n",
              "      <td>4</td>\n",
              "      <td>parameters of the architecture are summarized ...</td>\n",
              "      <td>2310.06825</td>\n",
              "      <td>Mistral 7B</td>\n",
              "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
              "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
              "      <td>Models and code are available at\\n  https://mi...</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.CL</td>\n",
              "      <td>20231010</td>\n",
              "      <td>20231010</td>\n",
              "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d267c8b-e3f4-4bd5-ada5-51f122b0c769')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4d267c8b-e3f4-4bd5-ada5-51f122b0c769 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4d267c8b-e3f4-4bd5-ada5-51f122b0c769');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-72f7168b-b87a-438d-b18b-b2b842225038\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-72f7168b-b87a-438d-b18b-b2b842225038')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-72f7168b-b87a-438d-b18b-b2b842225038 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = data[['chunk', 'source']]\n",
        "docs.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2ABC9w7g6gb0",
        "outputId": "807f3286-9dee-48c0-beb1-123aa68326bc"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               chunk  \\\n",
              "0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n",
              "1  automated benchmarks. Our models are released ...   \n",
              "2  GQA significantly accelerates the inference sp...   \n",
              "3  Mistral 7B takes a significant step in balanci...   \n",
              "4  parameters of the architecture are summarized ...   \n",
              "\n",
              "                            source  \n",
              "0  http://arxiv.org/pdf/2310.06825  \n",
              "1  http://arxiv.org/pdf/2310.06825  \n",
              "2  http://arxiv.org/pdf/2310.06825  \n",
              "3  http://arxiv.org/pdf/2310.06825  \n",
              "4  http://arxiv.org/pdf/2310.06825  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22392ebc-8c95-4394-98a5-36a3b773fc9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>automated benchmarks. Our models are released ...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GQA significantly accelerates the inference sp...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mistral 7B takes a significant step in balanci...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>parameters of the architecture are summarized ...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22392ebc-8c95-4394-98a5-36a3b773fc9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-22392ebc-8c95-4394-98a5-36a3b773fc9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-22392ebc-8c95-4394-98a5-36a3b773fc9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-eae8e9e3-a0d1-4745-a3aa-50593b531a76\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eae8e9e3-a0d1-4745-a3aa-50593b531a76')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-eae8e9e3-a0d1-4745-a3aa-50593b531a76 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "docs",
              "summary": "{\n  \"name\": \"docs\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"and reasoning benchmarks.\\n4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n3\\nFigure 4: Performance of Mistral 7B and different Llama models on a wide range of benchmarks . All\\nmodels were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mistral 7B\\nsignificantly outperforms Llama 2 7B and Llama 2 13B on all benchmarks. It is also vastly superior to Llama 1\\n34B in mathematics, code generation, and reasoning benchmarks.\\nModel Modality MMLU HellaSwag WinoG PIQA Arc-e Arc-c NQ TriviaQA HumanEval MBPP MATH GSM8K\\nLLaMA 2 7B Pretrained 44.4% 77.1% 69.5% 77.9% 68.7% 43.2% 24.7% 63.8% 11.6% 26.1% 3.9% 16.0%\\nLLaMA 2 13B Pretrained 55.6% 80.7% 72.9% 80.8% 75.2% 48.8% 29.0% 69.6% 18.9% 35.4% 6.0% 34.3%\",\n          \"in implementing new features and integrating their solutions into ours. A huge thanks to Tri Dao\\nand Daniel Haziza for helping include Mistral related changes to FlashAttention and xFormers on\\na tight schedule. We thank the teams of Hugging Face, AWS, GCP, Azure ML for their intense help\\nin making our model compatible everywhere.\\n6\\nFigure 6: Human evaluation of Mistral 7B \\u2013 Instruct vs Llama 2 13B \\u2013 Chat Example. An example of\\nhuman evaluation from llmboxing.com . The question asks for recommendations of books in quantum physics.\\nLlama 2 13B \\u2013 Chat recommends a general physics book, while Mistral 7B \\u2013 Instruct recommends a more\\nrelevant book on quantum physics and describes in the contents in more detail.\\n7\\nReferences\\n[1]Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr\\u00f3n, and\\nSumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head\\ncheckpoints. arXiv preprint arXiv:2305.13245 , 2023.\\n[2]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large\",\n          \"Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, L\\u00e9lio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\\u00e9e Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7\\u2013billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B \\u2013 Instruct, that surpasses Llama 2 13B \\u2013 chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"http://arxiv.org/pdf/2310.06825\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***RAG*** ✅"
      ],
      "metadata": {
        "id": "RmpHTHCe9J0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0nWA7Nh9a6F",
        "outputId": "7e2db5b3-6f26-4606-fcd5-1fa5d1026f71"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.8)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.19)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.86)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain_community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain_community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain_community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain_community) (2.20.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DataFrameLoader\n",
        "\n",
        "loader = DataFrameLoader(docs, page_content_column=\"chunk\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "NwtOqZUz87qP"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqJF-xOJ87m1",
        "outputId": "46207c63-a0cf-41ce-c7e5-6087253f54cc"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src')"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekUm3cRT87jc",
        "outputId": "acf39848-c417-4770-8832-b26746a1f041"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'http://arxiv.org/pdf/2310.06825'}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "url = os.getenv(\"QDRANT_URL\")\n",
        "api_key = os.getenv(\"QDRANT_KEY\")\n",
        "\n",
        "qdrant = Qdrant.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embeddings,\n",
        "    url=url,\n",
        "    collection_name=\"chatbot\",\n",
        "    api_key=api_key\n",
        ")"
      ],
      "metadata": {
        "id": "bSFDqQ6687gy"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is so special about Mistral 7B?\"\n",
        "qdrant.similarity_search(query, k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xQO64Vsryge",
        "outputId": "529773c1-8f1d-49b5-a696-8df4246b42e4"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': 'f510c7c1-9180-4796-9e63-d1fc66de1409', '_collection_name': 'chatbot'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src'),\n",
              " Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': 'bcab5251-f44f-4d67-825a-bc5c43043065', '_collection_name': 'chatbot'}, page_content='GQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\\nMistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more'),\n",
              " Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': '4aa5f46e-f4e7-4fa7-a2a5-37863eab6dbb', '_collection_name': 'chatbot'}, page_content='automated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nWebpage: https://mistral.ai/news/announcing-mistral-7b/\\n1 Introduction\\nIn the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\\nperformance often necessitates an escalation in model size. However, this scaling tends to increase\\ncomputational costs and inference latency, thereby raising barriers to deployment in practical,\\nreal-world scenarios. In this context, the search for balanced models delivering both high-level\\nperformance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\\na carefully designed language model can deliver high performance while maintaining an efficient\\ninference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\\nbenchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\\ngeneration. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during')]"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_prompt(query: str):\n",
        "    results = qdrant.similarity_search(query, k=3)\n",
        "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "    augment_prompt = f\"\"\"Using the contexts below, answer the query:\n",
        "\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "    return augment_prompt"
      ],
      "metadata": {
        "id": "0eKfSrTsryeY"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(custom_prompt(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghR2K5ierycZ",
        "outputId": "7698fca0-7afb-4f62-9c80-7a30324fb30f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the contexts below, answer the query:\n",
            "\n",
            "    Contexts:\n",
            "    Mistral 7B\n",
            "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
            "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
            "Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n",
            "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n",
            "William El Sayed\n",
            "Abstract\n",
            "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
            "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
            "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
            "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
            "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
            "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
            "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
            "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
            "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
            "Code: https://github.com/mistralai/mistral-src\n",
            "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
            "decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\n",
            "applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\n",
            "computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\n",
            "collectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\n",
            "Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\n",
            "implementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\n",
            "or Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\n",
            "also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\n",
            "a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\n",
            "model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\n",
            "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping\n",
            "large language models efficient. Through our work, our aim is to help the community create more\n",
            "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
            "Code: https://github.com/mistralai/mistral-src\n",
            "Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
            "1 Introduction\n",
            "In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\n",
            "performance often necessitates an escalation in model size. However, this scaling tends to increase\n",
            "computational costs and inference latency, thereby raising barriers to deployment in practical,\n",
            "real-world scenarios. In this context, the search for balanced models delivering both high-level\n",
            "performance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\n",
            "a carefully designed language model can deliver high performance while maintaining an efficient\n",
            "inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\n",
            "benchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\n",
            "generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\n",
            "without sacrificing performance on non-code related benchmarks.\n",
            "Mistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\n",
            "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
            "\n",
            "    Query: What is so special about Mistral 7B?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=custom_prompt(query)\n",
        ")\n",
        "\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat.invoke(messages)\n",
        "\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjVtjXEAryZ7",
        "outputId": "6315afca-e575-4bf4-b73c-21c2aa56eaf0"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistral 7B is a 7-billion-parameter language model developed to achieve superior performance and efficiency. Here are some key aspects that make Mistral 7B special:\n",
            "\n",
            "1. Superior Performance: Mistral 7B outperforms the best open 13B model (Llama 2) and the best released 34B model (Llama 1) across various benchmarks, including reasoning, mathematics, and code generation. It demonstrates high-level performance in diverse tasks, showcasing its versatility and effectiveness.\n",
            "\n",
            "2. Efficient Inference: Mistral 7B incorporates innovative attention mechanisms, such as Grouped-Query Attention (GQA) and Sliding Window Attention (SWA), to accelerate inference speed and reduce memory requirements during decoding. This allows for higher batch sizes and throughput, making it suitable for real-time applications.\n",
            "\n",
            "3. Open Source and Deployment: Mistral 7B is released under the Apache 2.0 license, enabling easy deployment locally or on cloud platforms like AWS, GCP, or Azure. A reference implementation is provided for streamlined integration with tools like vLLM and Hugging Face, making it accessible to the community for further development and fine-tuning.\n",
            "\n",
            "4. Balance of Performance and Efficiency: Mistral 7B represents a significant advancement in balancing high model performance with efficient inference. It demonstrates that a carefully designed language model can achieve top-tier performance while remaining practical and deployable in real-world scenarios.\n",
            "\n",
            "Overall, Mistral 7B stands out for its state-of-the-art design, exceptional performance across benchmarks, efficiency in inference, and commitment to open collaboration and development within the NLP community.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Groq**"
      ],
      "metadata": {
        "id": "7SDFGqDyuyyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81tGKV1DryX_",
        "outputId": "094c3392-e982-4063-ff0b-8d424e087d09"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.1.6-py3-none-any.whl (14 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.3,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from langchain-groq) (0.2.19)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain-groq) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain-groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain-groq) (0.1.86)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain-groq) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain-groq) (8.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain-groq) (3.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain-groq) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain-groq) (2.0.7)\n",
            "Installing collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.9.0 langchain-groq-0.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "chat = ChatGroq(\n",
        "    temperature=0,\n",
        "    model_name=\"mixtral-8x7b-32768\",\n",
        "    groq_api_key=\"groq_api_key\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "OE2uhQvQryTp"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=custom_prompt(query)\n",
        ")\n",
        "\n",
        "messages.append(prompt)\n",
        "res = chat.invoke(messages)\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBRMFMRQvvb4",
        "outputId": "47d03d87-565b-48a1-d4a6-260ad0d9b324"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistral 7B is a 7-billion-parameter language model that is engineered for superior performance and efficiency. It outperforms the best open 13B model (Llama 2) across all evaluated benchmarks and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Mistral 7B uses two attention mechanisms: grouped-query attention (GQA) and sliding window attention (SWA). GQA significantly accelerates the inference speed and reduces memory requirements during decoding, allowing for higher batch sizes and throughput. SWA handles longer sequences more effectively at a reduced computational cost. These attention mechanisms contribute to the enhanced performance and efficiency of Mistral 7B. Additionally, Mistral 7B is released under the Apache 2.0 license, and a reference implementation is provided for easy deployment on cloud platforms or local systems. Mistral 7B is also designed for ease of fine-tuning across a myriad of tasks, and a chat model fine-tuned from Mistral 7B significantly outperforms the Llama 2 13B – Chat model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Regards, Tariq Ibrahim*"
      ],
      "metadata": {
        "id": "RkH_YORrw6W7"
      }
    }
  ]
}